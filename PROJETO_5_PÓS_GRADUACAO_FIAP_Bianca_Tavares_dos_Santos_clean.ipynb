{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceae3f16",
   "metadata": {
    "id": "bIf8qKNRCDpJ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd81e032",
   "metadata": {
    "id": "5jTgcgH-C3ch"
   },
   "source": [
    "# MOMENTO 1 — Importação e organização dos insumos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16ad5ca",
   "metadata": {
    "id": "x8aqLlnuej-T"
   },
   "source": [
    "## ETAPA 0 — Setup do ambiente + imports centralizados\n",
    "**Resumo:** Preparar dependências, padronizar imports e iniciar sessão Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786d1d67",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kvm-iAkHC_Xy",
    "outputId": "3c940006-42af-427f-c38b-27c48651021f"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ETAPA 13.0 — Setup Streamlit no Google Colab\n",
    "# =========================\n",
    "\n",
    "!pip install -q streamlit pyngrok pandas numpy\n",
    "!pip -q install streamlit pyarrow pandas numpy\n",
    "!npm -q install -g localtunnel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927239e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ju9X6TAMN_JM",
    "outputId": "ffff35d8-44ec-44bb-c1df-197f9eef163c"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ETAPA 0 — Setup (instalação opcional) + Imports centralizados\n",
    "# =========================\n",
    "\n",
    "# --- 0.0: Instalação (ative se estiver em ambiente que precisa) ---\n",
    "# !pip -q install pyspark ijson\n",
    "!pip install -q streamlit pyngrok pandas numpy\n",
    "\n",
    "\n",
    "# --- 0.1: Python stdlib ---\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 0.2: Data / Viz (se você usar mesmo; se não usar, pode remover) ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns  # (opcional) se não estiver usando, remova para evitar dependência extra\n",
    "\n",
    "# --- 0.3: JSON streaming (Etapas de leitura) ---\n",
    "import ijson\n",
    "\n",
    "# --- 0.4: Spark SQL core ---\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import (\n",
    "    StringType, IntegerType, DoubleType, BooleanType,\n",
    "    StructType, StructField, ArrayType\n",
    ")\n",
    "\n",
    "# --- 0.5: Spark ML helpers ---\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# Transformers / feature engineering\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, OneHotEncoder, VectorAssembler,\n",
    "    Tokenizer, RegexTokenizer, StopWordsRemover, HashingTF, IDF,\n",
    "    StandardScaler\n",
    ")\n",
    "\n",
    "# Models\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Split / Tuning (se você usar depois)\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Evaluators / métricas\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# --- 0.6: Spark session (se você cria aqui; se já existe no ambiente, pode comentar) ---\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Projeto-ML-RH\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"[Etapa 0] Setup OK | Spark:\", spark.version)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d22cda0",
   "metadata": {
    "id": "Dcb3lF4gewrO"
   },
   "source": [
    "## ETAPA 1 — Ingestão dos dados no Colab e padronização de caminhos\n",
    "**Resumo:** upload de arquivos do  Google Drive, organizar paths de entrada/saída e preparar conversão JSON → JSONL → Parquet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec39de6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2GKADrHYxgCN",
    "outputId": "584eb1d3-acb0-463b-c5d3-20e87d817d50"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "## ETAPA 1 -- INICIO DE AJUSTES DOS DADOS UTILIZADOS NO PROJETO\n",
    "# Buscando arquivos (bases compartilhadas para elaboração do projeto) para abrir (Google Drive) e realizar modificações\n",
    "# Objetivo final: migrar o tipo de arquivo de JSON > JSONL > PARQUET ==> melhor modelagem para desenvolvimento de schemas de dados entre bases (Spark)\n",
    "# =========================\n",
    "\n",
    "# puxando arquivos na pasta do GDrive para elaboração do projeto\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# puxando PASTA DO PROJETO (-- obs: criando uma pasta apenas para outputs)\n",
    "BASE_DIR = \"/content/drive/MyDrive/Pós Tech - Data Analytics & ML - FIAP/FASE 5/Tech Challenge - Fase 5\"\n",
    "DATA_DIR = f\"{BASE_DIR}/Base de Dados\"\n",
    "OUT_DIR  = f\"{BASE_DIR}/_outputs\"\n",
    "\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ENTRADA: Arquivos en JSON\n",
    "APPLICANTS_JSON = f\"{DATA_DIR}/applicants.json\"\n",
    "PROSPECTS_JSON  = f\"{DATA_DIR}/prospects.json\"\n",
    "VAGAS_JSON      = f\"{DATA_DIR}/vagas.json\"\n",
    "\n",
    "# SAÍDA 0 (intermediária):  1ª camada de ajustes: JSON > JSONL (arquivos JSON dict, organizados por ID)\n",
    "APPLICANTS_JSONL = f\"{OUT_DIR}/applicants.jsonl\"\n",
    "PROSPECTS_JSONL  = f\"{OUT_DIR}/prospects.jsonl\"\n",
    "VAGAS_JSONL      = f\"{OUT_DIR}/vagas.jsonl\"\n",
    "\n",
    "# SAÍDA FINAL: 2ª camada de ajustes: JSON > JSONL (arquivos JSON dict, organizados por ID)\n",
    "APPLICANTS_PARQUET = f\"{OUT_DIR}/applicants_parquet\"\n",
    "PROSPECTS_PARQUET  = f\"{OUT_DIR}/prospects_parquet\"\n",
    "VAGAS_PARQUET      = f\"{OUT_DIR}/vagas_parquet\"\n",
    "\n",
    "# Validação\n",
    "for p in [APPLICANTS_JSON, PROSPECTS_JSON, VAGAS_JSON]:\n",
    "    print(\"exists?\", os.path.exists(p), \"|\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89d1afa",
   "metadata": {
    "id": "Sqt6fGEQCUF9"
   },
   "source": [
    "# MOMENTO 2 — Engenharia de dados e estruturação em Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7412e6",
   "metadata": {
    "id": "Kn6hno22e1mB"
   },
   "source": [
    "\n",
    "## ETAPA 2 — Carregamento dos Parquets e definição dos DataFrames-base\n",
    "**Resumo:** ler Parquet no Spark, criar dataframes principais e validar volume/esquema antes das transformações.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc22689",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OLTEL43EKFK5",
    "outputId": "eae5c207-7939-4ec2-ba70-0771d177b5d8"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "## ETAPA 2: SEPARANDO OS ARQUIVOS PARQUET E DEFININDO PRINCIPAIS DATAFRAMES PARA O PROJETO\n",
    "# =========================\n",
    "\n",
    "\n",
    "# --- Carregar Parquets ---\n",
    "df_prospects = spark.read.parquet(PROSPECTS_PARQUET)\n",
    "df_vagas     = spark.read.parquet(VAGAS_PARQUET)\n",
    "df_apps      = spark.read.parquet(APPLICANTS_PARQUET)\n",
    "\n",
    "# Teste de Sanidade\n",
    "print(\"prospects:\", df_prospects.count(), \"linhas\")\n",
    "print(\"vagas:\", df_vagas.count(), \"linhas\")\n",
    "print(\"applicants:\", df_apps.count(), \"linhas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c175382",
   "metadata": {
    "id": "uJkEw3GJe5jI"
   },
   "source": [
    "## ETAPA 3 — Normalização e consolidação da base prospects (para join/ML)\n",
    "**Resumo:** achatar estruturas aninhadas, definir chave **única** e consolidar registros para formar uma base consistente para modelagem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631d2a1d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q7N-BG-0Rlld",
    "outputId": "de305f56-edec-4067-e5b1-7c59fb1912b4"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ETAPA 3 — NORMALIZAÇÃO + CHAVE + CARDINALIDADE + CONSOLIDAÇÃO (UNIFICADA e RESUMIDA)\n",
    "# Inputs esperados:\n",
    "#   - df_prospects (schema: __root_id, modalidade, titulo, prospects: array<struct{codigo,...}>)\n",
    "#   - (opcional depois) df_apps, df_vagas para join na Etapa 4\n",
    "# Outputs:\n",
    "#   - df_pro_flat  : 1 linha por item do array prospects (explode)\n",
    "#   - df_pro_model : 1 linha por codigo (consolidado p/ join/ML)\n",
    "# =========================\n",
    "\n",
    "\n",
    "# 3.1 — Inspeção mínima (schema + colunas)\n",
    "print(\"\\n[Etapa 3.1] Columns df_prospects:\", df_prospects.columns)\n",
    "df_prospects.printSchema()\n",
    "\n",
    "# 3.2 — Flatten (explode) + seleção mínima de campos úteis (normalização estrutural)\n",
    "df_pro_flat = (\n",
    "    df_prospects\n",
    "    .select(\"__root_id\",\"modalidade\",\"titulo\", F.explode_outer(\"prospects\").alias(\"p\"))\n",
    "    .select(\n",
    "        \"__root_id\",\"modalidade\",\"titulo\",\n",
    "        F.col(\"p.codigo\").alias(\"codigo\"),\n",
    "        F.col(\"p.nome\").alias(\"nome\"),\n",
    "        F.col(\"p.recrutador\").alias(\"recrutador\"),\n",
    "        F.col(\"p.data_candidatura\").alias(\"data_candidatura\"),\n",
    "        F.col(\"p.situacao_candidado\").alias(\"situacao_candidado\"),\n",
    "        F.col(\"p.ultima_atualizacao\").alias(\"ultima_atualizacao\"),\n",
    "        F.col(\"p.comentario\").alias(\"comentario\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 3.3 — Diagnóstico de cardinalidade (prints curtos e objetivos)\n",
    "print(\"\\n[Etapa 3.3] Top __root_id counts (estrutura do JSON):\")\n",
    "df_pro_flat.groupBy(\"__root_id\").count().orderBy(F.desc(\"count\")).show(5, truncate=False)\n",
    "\n",
    "print(\"\\n[Etapa 3.3] Top codigo counts (duplicidade/nulos da chave candidata):\")\n",
    "df_pro_flat.groupBy(\"codigo\").count().orderBy(F.desc(\"count\")).show(10, truncate=False)\n",
    "\n",
    "print(\"\\n[Etapa 3.3] Sanidade codigo:\")\n",
    "print(\"rows(flat) =\", df_pro_flat.count())\n",
    "print(\"null(codigo) =\", df_pro_flat.where(F.col(\"codigo\").isNull()).count())\n",
    "print(\"distinct(codigo, non-null) =\", df_pro_flat.where(F.col(\"codigo\").isNotNull()).select(\"codigo\").distinct().count())\n",
    "\n",
    "# 3.4 — Consolidação: 1 linha por codigo (mantém registro mais recente por ultima_atualizacao)\n",
    "# CORREÇÃO: usar try_to_timestamp via expr (maior compatibilidade com versões Spark)\n",
    "\n",
    "df_tmp = (\n",
    "    df_pro_flat\n",
    "    .where(F.col(\"codigo\").isNotNull())\n",
    "    .withColumn(\n",
    "        \"ultima_atualizacao_ts\",\n",
    "        F.coalesce(\n",
    "            F.expr(\"try_to_timestamp(ultima_atualizacao, 'dd-MM-yyyy HH:mm:ss')\"),\n",
    "            F.expr(\"try_to_timestamp(ultima_atualizacao, 'dd-MM-yyyy')\"),\n",
    "            F.expr(\"try_to_timestamp(ultima_atualizacao, 'yyyy-MM-dd HH:mm:ss')\"),\n",
    "            F.expr(\"try_to_timestamp(ultima_atualizacao, 'yyyy-MM-dd')\")\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"data_candidatura_ts\",\n",
    "        F.coalesce(\n",
    "            F.expr(\"try_to_timestamp(data_candidatura, 'dd-MM-yyyy HH:mm:ss')\"),\n",
    "            F.expr(\"try_to_timestamp(data_candidatura, 'dd-MM-yyyy')\"),\n",
    "            F.expr(\"try_to_timestamp(data_candidatura, 'yyyy-MM-dd HH:mm:ss')\"),\n",
    "            F.expr(\"try_to_timestamp(data_candidatura, 'yyyy-MM-dd')\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"codigo\").orderBy(F.col(\"ultima_atualizacao_ts\").desc_nulls_last())\n",
    "\n",
    "df_pro_model = (\n",
    "    df_tmp\n",
    "    .withColumn(\"rn\", F.row_number().over(w))\n",
    "    .where(F.col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "# 3.5 — Output final da Etapa 3 (pronto para join/ML)\n",
    "print(\"\\n[Etapa 3] OUTPUTS:\")\n",
    "print(\"df_pro_flat rows =\", df_pro_flat.count())\n",
    "print(\"df_pro_model rows (1 linha por codigo) =\", df_pro_model.count())\n",
    "\n",
    "print(\"\\n[Etapa 3] Sanidade datas (após parsing):\")\n",
    "print(\"null(ultima_atualizacao_ts) =\", df_tmp.where(F.col(\"ultima_atualizacao_ts\").isNull()).count())\n",
    "print(\"null(data_candidatura_ts) =\", df_tmp.where(F.col(\"data_candidatura_ts\").isNull()).count())\n",
    "\n",
    "df_pro_model.select(\n",
    "    \"codigo\",\"nome\",\"titulo\",\"modalidade\",\n",
    "    \"situacao_candidado\",\"ultima_atualizacao\",\"ultima_atualizacao_ts\"\n",
    ").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210192c7",
   "metadata": {
    "id": "oWO95uYTe7Xk"
   },
   "source": [
    "## ETAPA 4 — Join e criação da base unificada (Gold)\n",
    "**Resumo:** unir candidatos (apps/prospects) com vagas, padronizar chaves/colunas e gerar a base final para feature engineering e modelagem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f1f10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rVlSgJKHVHjy",
    "outputId": "37a44c20-8652-4a59-98dd-f996864edfa5"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ETAPA 4 — JOIN + BASE GOLD (UNIFICADA e RESUMIDA)\n",
    "# Inputs:\n",
    "#   - df_pro_model  (Etapa 3)  -> deve ter coluna \"codigo\" (string/int ok, vamos cast)\n",
    "#   - df_apps       (candidaturas/applicants)\n",
    "#   - df_vagas      (vagas/jobs)\n",
    "# Output:\n",
    "#   - df_gold       (base unificada p/ feature engineering / ML)\n",
    "# =========================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 4.1 — Inspeção mínima (para não chutar nomes)\n",
    "print(\"\\n[Etapa 4.1] df_apps columns:\", df_apps.columns)\n",
    "df_apps.printSchema()\n",
    "\n",
    "print(\"\\n[Etapa 4.1] df_vagas columns:\", df_vagas.columns)\n",
    "df_vagas.printSchema()\n",
    "\n",
    "# 4.2 — Função curta para sugerir chaves por nome (heurística)\n",
    "def pick_key(cols, patterns):\n",
    "    for p in patterns:\n",
    "        for c in cols:\n",
    "            if p in c.lower():\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "# 4.3 — Definir chaves\n",
    "pro_key = \"codigo\"  # fixo da Etapa 3\n",
    "\n",
    "# candidata chave de candidato em df_apps\n",
    "app_key = pick_key(df_apps.columns, [\n",
    "    \"codigo\", \"candidate\", \"candidato\", \"applicant\", \"prospect\", \"id\"\n",
    "])\n",
    "\n",
    "# candidata chave de vaga em df_apps e df_vagas\n",
    "job_keyA = pick_key(df_apps.columns,  [\"vaga\", \"job\", \"position\", \"requisition\", \"id_vaga\", \"job_id\"])\n",
    "job_keyV = pick_key(df_vagas.columns, [\"vaga\", \"job\", \"position\", \"requisition\", \"id_vaga\", \"job_id\"])\n",
    "\n",
    "print(\"\\n[Etapa 4.3] Chaves sugeridas:\")\n",
    "print(\"pro_key  =\", pro_key)\n",
    "print(\"app_key  =\", app_key)\n",
    "print(\"job_keyA =\", job_keyA)\n",
    "print(\"job_keyV =\", job_keyV)\n",
    "\n",
    "# 4.4 — Normalização leve de tipos (string) para reduzir mismatch no join\n",
    "df_pro = df_pro_model.withColumn(pro_key, F.col(pro_key).cast(\"string\"))\n",
    "df_app = df_apps\n",
    "df_vag = df_vagas\n",
    "\n",
    "if app_key:\n",
    "    df_app = df_app.withColumn(app_key, F.col(app_key).cast(\"string\"))\n",
    "if job_keyA:\n",
    "    df_app = df_app.withColumn(job_keyA, F.col(job_keyA).cast(\"string\"))\n",
    "if job_keyV:\n",
    "    df_vag = df_vag.withColumn(job_keyV, F.col(job_keyV).cast(\"string\"))\n",
    "\n",
    "# 4.5 — JOIN prospects ↔ apps (obrigatório) COM DESAMBIGUAÇÃO DE COLUNAS\n",
    "# Fix: usar alias + selecionar colunas com prefixo p__/app__ para evitar nomes duplicados\n",
    "p = df_pro.alias(\"p\")\n",
    "a = df_app.alias(\"a\")\n",
    "\n",
    "df_join_pa = (\n",
    "    p.join(a, F.col(f\"p.{pro_key}\") == F.col(f\"a.{app_key}\"), \"left\")\n",
    "     .select(\n",
    "         *[F.col(f\"p.{c}\").alias(c) for c in df_pro.columns],                       # mantém prospects sem prefixo\n",
    "         *[F.col(f\"a.{c}\").alias(f\"app__{c}\") for c in df_app.columns]              # prefixa tudo de apps\n",
    "     )\n",
    ")\n",
    "\n",
    "# 4.6 — JOIN com vagas (opcional) COM DESAMBIGUAÇÃO\n",
    "if job_keyA and job_keyV:\n",
    "    j = df_join_pa.alias(\"j\")\n",
    "    v = df_vag.alias(\"v\")\n",
    "\n",
    "    df_gold = (\n",
    "        j.join(v, F.col(f\"j.{job_keyA}\") == F.col(f\"v.{job_keyV}\"), \"left\")\n",
    "         .select(\n",
    "             *[F.col(f\"j.{c}\").alias(c) for c in df_join_pa.columns],               # mantém tudo do join_pa\n",
    "             *[F.col(f\"v.{c}\").alias(f\"vag__{c}\") for c in df_vag.columns]          # prefixa tudo de vagas\n",
    "         )\n",
    "    )\n",
    "else:\n",
    "    print(\"\\n[Etapa 4.6] Aviso: não identifiquei chave de vaga em df_apps e/ou df_vagas. df_gold ficará sem join de vagas.\")\n",
    "    df_gold = df_join_pa\n",
    "\n",
    "# 4.7 — Sanity check final (agora SEM ambiguidade)\n",
    "print(\"\\n[Etapa 4.7] Sanidade df_gold:\")\n",
    "print(\"rows(df_pro_model) =\", df_pro_model.count())\n",
    "print(\"rows(df_gold)      =\", df_gold.count())\n",
    "print(\"distinct(codigo)   =\", df_gold.select(pro_key).distinct().count())\n",
    "\n",
    "# taxa de match: como apps foi prefixado, a chave do apps virou app__{app_key}\n",
    "app_key_pref = f\"app__{app_key}\"\n",
    "\n",
    "match_apps = (\n",
    "    df_gold\n",
    "    .withColumn(\"matched_apps\", F.when(F.col(app_key_pref).isNotNull(), F.lit(1)).otherwise(F.lit(0)))\n",
    "    .groupBy(pro_key)\n",
    "    .agg(F.max(\"matched_apps\").alias(\"has_app\"))\n",
    "    .agg(F.avg(\"has_app\").alias(\"pct_match_apps\"))\n",
    "    .collect()[0][\"pct_match_apps\"]\n",
    ")\n",
    "\n",
    "print(\"pct_match_apps ≈\", float(match_apps))\n",
    "\n",
    "# Amostra enxuta (somente colunas garantidas do prospects + flag + chave prefixada)\n",
    "df_gold.select(\n",
    "    pro_key, \"nome\", \"titulo\", \"situacao_candidado\", \"ultima_atualizacao\", app_key_pref\n",
    ").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c96a3da",
   "metadata": {
    "id": "-KfmSwTQDNCK"
   },
   "source": [
    "# MOMENTO 3 — Feature Engineering e preparação para modelagem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e2958e",
   "metadata": {
    "id": "j4POpC_bfA2Z"
   },
   "source": [
    "## ETAPA 5 — Feature Engineering (dataset de modelagem)\n",
    "**Resumo:** criar target (y) e transformar a base Gold em uma tabela tabular com features prontas para treino e avaliação.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4315fbeb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FSbB2z7VZpX9",
    "outputId": "d24f8e08-3e02-4612-9441-dd2dd2988565"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ETAPA 5 — FEATURE ENGINEERING (UNIFICADA e RESUMIDA)\n",
    "# Input:\n",
    "#   - df_gold (saída da Etapa 4, com colunas do prospects + app__ structs)\n",
    "# Output:\n",
    "#   - df_ml (dataset tabular com label y + features prontas p/ ML)\n",
    "# =========================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "import re\n",
    "\n",
    "# 5.1 — Target (y) a partir do status (situacao_candidado) — ajuste a lista se quiser\n",
    "pos_patterns = [\n",
    "    \"contratad\", \"aprovad\", \"hunting\", \"admit\", \"selecionad\"\n",
    "]\n",
    "pos_regex = \"|\".join([re.escape(p) for p in pos_patterns])\n",
    "\n",
    "df_feat = (\n",
    "    df_gold\n",
    "    .withColumn(\"status_lc\", F.lower(F.coalesce(F.col(\"situacao_candidado\"), F.lit(\"\"))))\n",
    "    .withColumn(\"y\", F.when(F.col(\"status_lc\").rlike(pos_regex), F.lit(1)).otherwise(F.lit(0)))\n",
    ")\n",
    "\n",
    "# 5.2 — Flatten mínimo do df_apps (structs -> colunas simples) e limpeza leve\n",
    "# (Só campos muito úteis e de baixo custo; evita explodir o dataset)\n",
    "df_feat = (\n",
    "    df_feat\n",
    "    .withColumn(\"app_nome\", F.col(\"app__informacoes_pessoais.nome\"))\n",
    "    .withColumn(\"app_email\", F.coalesce(F.col(\"app__informacoes_pessoais.email\"), F.col(\"app__infos_basicas.email\")))\n",
    "    .withColumn(\"app_local\", F.coalesce(F.col(\"app__infos_basicas.local\"), F.col(\"app__informacoes_pessoais.endereco\")))\n",
    "    .withColumn(\"app_nivel_academico\", F.col(\"app__formacao_e_idiomas.nivel_academico\"))\n",
    "    .withColumn(\"app_nivel_ingles\", F.col(\"app__formacao_e_idiomas.nivel_ingles\"))\n",
    "    .withColumn(\"app_nivel_espanhol\", F.col(\"app__formacao_e_idiomas.nivel_espanhol\"))\n",
    "    .withColumn(\"app_area_atuacao\", F.col(\"app__informacoes_profissionais.area_atuacao\"))\n",
    "    .withColumn(\"app_nivel_profissional\", F.col(\"app__informacoes_profissionais.nivel_profissional\"))\n",
    "    .withColumn(\"app_remuneracao_raw\", F.col(\"app__informacoes_profissionais.remuneracao\"))\n",
    "    .withColumn(\"app_conhecimentos_raw\", F.col(\"app__informacoes_profissionais.conhecimentos_tecnicos\"))\n",
    "    .withColumn(\"app_certificacoes_raw\", F.col(\"app__informacoes_profissionais.certificacoes\"))\n",
    ")\n",
    "\n",
    "# 5.3 — Features simples e robustas (sem parsing complexo de salário por enquanto)\n",
    "df_feat = (\n",
    "    df_feat\n",
    "    .withColumn(\"titulo_len\", F.length(F.coalesce(F.col(\"titulo\"), F.lit(\"\"))))\n",
    "    .withColumn(\"nome_len\", F.length(F.coalesce(F.col(\"nome\"), F.lit(\"\"))))\n",
    "    .withColumn(\"has_email\", F.when(F.col(\"app_email\").isNotNull() & (F.length(F.col(\"app_email\")) > 3), 1).otherwise(0))\n",
    "    .withColumn(\"has_linkedin\", F.when(F.col(\"app__informacoes_pessoais.url_linkedin\").isNotNull(), 1).otherwise(0))\n",
    "    .withColumn(\"has_cv_pt\", F.when(F.col(\"app__cv_pt\").isNotNull() & (F.length(F.col(\"app__cv_pt\")) > 0), 1).otherwise(0))\n",
    "    .withColumn(\"has_cv_en\", F.when(F.col(\"app__cv_en\").isNotNull() & (F.length(F.col(\"app__cv_en\")) > 0), 1).otherwise(0))\n",
    "    .withColumn(\"has_conhecimentos\", F.when(F.col(\"app_conhecimentos_raw\").isNotNull() & (F.length(F.col(\"app_conhecimentos_raw\")) > 0), 1).otherwise(0))\n",
    "    .withColumn(\"has_certificacoes\", F.when(F.col(\"app_certificacoes_raw\").isNotNull() & (F.length(F.col(\"app_certificacoes_raw\")) > 0), 1).otherwise(0))\n",
    ")\n",
    "\n",
    "# 5.4 — Seleção final do dataset para ML (tabular + textos raw como opcional)\n",
    "df_ml = df_feat.select(\n",
    "    F.col(\"codigo\").cast(\"string\").alias(\"candidate_id\"),\n",
    "    \"y\",\n",
    "    \"modalidade\",\n",
    "    \"titulo\",\n",
    "    \"status_lc\",\n",
    "    \"ultima_atualizacao\",\n",
    "    \"titulo_len\",\n",
    "    \"nome_len\",\n",
    "    \"has_email\",\n",
    "    \"has_linkedin\",\n",
    "    \"has_cv_pt\",\n",
    "    \"has_cv_en\",\n",
    "    \"has_conhecimentos\",\n",
    "    \"has_certificacoes\",\n",
    "    \"app_local\",\n",
    "    \"app_nivel_academico\",\n",
    "    \"app_nivel_profissional\",\n",
    "    \"app_area_atuacao\",\n",
    "    \"app_nivel_ingles\",\n",
    "    \"app_nivel_espanhol\",\n",
    "    # textos raw (mantém pra NLP futura, mas não quebra nada agora)\n",
    "    \"app_conhecimentos_raw\",\n",
    "    \"app_certificacoes_raw\",\n",
    "    \"app_remuneracao_raw\"\n",
    ")\n",
    "\n",
    "# 5.5 — Sanity check curto\n",
    "print(\"\\n[Etapa 5] Sanidade df_ml:\")\n",
    "print(\"rows =\", df_ml.count())\n",
    "df_ml.groupBy(\"y\").count().show(truncate=False)\n",
    "df_ml.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c3d17d",
   "metadata": {
    "id": "b4X6HBfH6eS7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd39cc3b",
   "metadata": {
    "id": "wnqBFYBVDvpt"
   },
   "source": [
    "# MOMENTO 4 — Modelagem e avaliação analítica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e7de0e",
   "metadata": {
    "id": "dzfm5XnVfDWk"
   },
   "source": [
    "## ETAPA 6 — Modelagem e racional analítico\n",
    "**Resumo:** Nessa etapa, é necessário introduzir sobre o contexto lógico de escolha do modelo, abordagem de treinamento e interpretação dos resultados, considerando o RH como principal stakeholder.\n",
    "\n",
    "\n",
    "\n",
    "*Inicialmente, foi desenvolvido um modelo generalista utilizando todos os candidatos disponíveis, de modo a capturar padrões globais de sucesso no processo seletivo. Reconhece-se, entretanto, que diferentes áreas profissionais apresentam características e critérios distintos, motivo pelo qual análises segmentadas por área são propostas como extensão do trabalho.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4968134",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YlP1GKieaA4J",
    "outputId": "23ec8975-7460-4eef-b7b5-eb28cb7987af"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ETAPA 6 — MODELAGEM BASELINE (ROBUSTA) SEM ONEHOT\n",
    "# Motivo: evitar erros de metadata/nomes no OneHotEncoder (\"Cannot have an empty string for name\" etc.)\n",
    "# Input:\n",
    "#   - df_ml (Etapa 5)\n",
    "# Output:\n",
    "#   - model, predictions, métricas\n",
    "# =========================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "label_col = \"y\"\n",
    "\n",
    "num_cols = [\n",
    "    \"titulo_len\",\n",
    "    \"nome_len\",\n",
    "    \"has_email\",\n",
    "    \"has_linkedin\",\n",
    "    \"has_cv_pt\",\n",
    "    \"has_cv_en\",\n",
    "    \"has_conhecimentos\",\n",
    "    \"has_certificacoes\"\n",
    "]\n",
    "\n",
    "cat_cols = [\n",
    "    \"modalidade\",\n",
    "    \"app_local\",\n",
    "    \"app_nivel_academico\",\n",
    "    \"app_nivel_profissional\",\n",
    "    \"app_area_atuacao\",\n",
    "    \"app_nivel_ingles\",\n",
    "    \"app_nivel_espanhol\"\n",
    "]\n",
    "\n",
    "# 6.1 — Dataset base\n",
    "df_model = df_ml.select(label_col, *num_cols, *cat_cols)\n",
    "\n",
    "# 6.2 — Normalização forte (garante que NUNCA tem null/\"\")\n",
    "# numéricas: double + null -> 0\n",
    "for c in num_cols:\n",
    "    df_model = df_model.withColumn(c, F.coalesce(F.col(c).cast(\"double\"), F.lit(0.0)))\n",
    "\n",
    "# categóricas: trim + null/\" \" -> \"unknown\"\n",
    "for c in cat_cols:\n",
    "    df_model = df_model.withColumn(\n",
    "        c,\n",
    "        F.when(\n",
    "            F.trim(F.coalesce(F.col(c).cast(\"string\"), F.lit(\"\"))) == \"\",\n",
    "            F.lit(\"unknown\")\n",
    "        ).otherwise(F.trim(F.col(c).cast(\"string\")))\n",
    "    )\n",
    "\n",
    "# (debug mínimo — pode comentar depois)\n",
    "print(\"\\n[Etapa 6] Checagem de strings vazias após limpeza:\")\n",
    "for c in cat_cols:\n",
    "    empties = df_model.where(F.col(c) == \"\").count()\n",
    "    nulls = df_model.where(F.col(c).isNull()).count()\n",
    "    print(f\"- {c}: empty={empties}, null={nulls}\")\n",
    "\n",
    "# 6.3 — Indexação categórica (sem OneHot)\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n",
    "    for c in cat_cols\n",
    "]\n",
    "\n",
    "# 6.4 — VectorAssembler (numéricas + idx)\n",
    "feature_cols = num_cols + [f\"{c}_idx\" for c in cat_cols]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# 6.5 — Modelo baseline\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=label_col,\n",
    "    maxIter=50,\n",
    "    regParam=0.01\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + [assembler, lr])\n",
    "\n",
    "# 6.6 — Split\n",
    "train_df, test_df = df_model.randomSplit([0.7, 0.3], seed=42)\n",
    "print(\"\\n[Etapa 6] Split:\")\n",
    "print(\"train =\", train_df.count())\n",
    "print(\"test  =\", test_df.count())\n",
    "\n",
    "# 6.7 — Treino\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# 6.8 — Predição\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# 6.9 — Métricas\n",
    "auc_eval = BinaryClassificationEvaluator(\n",
    "    labelCol=label_col,\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "acc_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=label_col,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "prec_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=label_col,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedPrecision\"\n",
    ")\n",
    "\n",
    "rec_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=label_col,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedRecall\"\n",
    ")\n",
    "\n",
    "auc = auc_eval.evaluate(predictions)\n",
    "acc = acc_eval.evaluate(predictions)\n",
    "prec = prec_eval.evaluate(predictions)\n",
    "rec = rec_eval.evaluate(predictions)\n",
    "\n",
    "print(\"\\n[Etapa 6] Métricas:\")\n",
    "print(f\"AUC       = {auc:.4f}\")\n",
    "print(f\"Accuracy  = {acc:.4f}\")\n",
    "print(f\"Precision = {prec:.4f}\")\n",
    "print(f\"Recall    = {rec:.4f}\")\n",
    "\n",
    "# 6.10 — Amostra de predições\n",
    "predictions.select(label_col, \"prediction\", \"probability\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16818437",
   "metadata": {
    "id": "qB07b0mHfFAs"
   },
   "source": [
    "## ETAPA 7 — Avaliação do modelo e análise de métricas\n",
    "**Resumo:** Avaliar o desempenho do modelo por métricas estatísticas e interpretar os resultados no contexto de decisão do RH.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863935ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NRoV_xaY1ZBi",
    "outputId": "8b1d6efb-1d55-4a28-d1c8-0b6e826e74a3"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ETAPA 7 — AVALIAÇÃO CORRETA (DESBALANCEAMENTO + CONFUSION + THRESHOLD)\n",
    "# Input:\n",
    "#   - predictions (saída da Etapa 6)\n",
    "#   - df_model (usado na Etapa 6) OU df_ml (para distribuição do y)\n",
    "# Output:\n",
    "#   - distribuição do y\n",
    "#   - confusion matrix\n",
    "#   - precision/recall/f1 da classe positiva (y=1)\n",
    "#   - sugestão de threshold (opcional)\n",
    "# =========================\n",
    "\n",
    "# 7.1 — Distribuição do target no dataset todo\n",
    "print(\"\\n[Etapa 7.1] Distribuição do y (dataset completo):\")\n",
    "df_ml.groupBy(\"y\").count().orderBy(\"y\").show()\n",
    "\n",
    "# 7.2 — Confusion Matrix no TEST (predictions)\n",
    "# tn, fp, fn, tp\n",
    "cm = (\n",
    "    predictions\n",
    "    .select(\"y\", \"prediction\")\n",
    "    .withColumn(\"y_int\", F.col(\"y\").cast(\"int\"))\n",
    "    .withColumn(\"pred_int\", F.col(\"prediction\").cast(\"int\"))\n",
    "    .groupBy(\"y_int\", \"pred_int\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "print(\"\\n[Etapa 7.2] Confusion Matrix (test):\")\n",
    "cm.orderBy(\"y_int\", \"pred_int\").show()\n",
    "\n",
    "# 7.3 — Métricas manuais focadas na classe positiva (y=1)\n",
    "agg = (\n",
    "    predictions\n",
    "    .select(F.col(\"y\").cast(\"int\").alias(\"y\"), F.col(\"prediction\").cast(\"int\").alias(\"pred\"))\n",
    "    .agg(\n",
    "        F.sum(F.when((F.col(\"y\")==1) & (F.col(\"pred\")==1), 1).otherwise(0)).alias(\"tp\"),\n",
    "        F.sum(F.when((F.col(\"y\")==0) & (F.col(\"pred\")==1), 1).otherwise(0)).alias(\"fp\"),\n",
    "        F.sum(F.when((F.col(\"y\")==1) & (F.col(\"pred\")==0), 1).otherwise(0)).alias(\"fn\"),\n",
    "        F.sum(F.when((F.col(\"y\")==0) & (F.col(\"pred\")==0), 1).otherwise(0)).alias(\"tn\"),\n",
    "    )\n",
    ").collect()[0]\n",
    "\n",
    "tp, fp, fn, tn = agg[\"tp\"], agg[\"fp\"], agg[\"fn\"], agg[\"tn\"]\n",
    "\n",
    "precision_pos = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "recall_pos    = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "f1_pos        = (2 * precision_pos * recall_pos / (precision_pos + recall_pos)) if (precision_pos + recall_pos) else 0.0\n",
    "\n",
    "print(\"\\n[Etapa 7.3] Métricas para classe positiva (y=1):\")\n",
    "print(f\"TP={tp}  FP={fp}  FN={fn}  TN={tn}\")\n",
    "print(f\"Precision(y=1) = {precision_pos:.4f}\")\n",
    "print(f\"Recall(y=1)    = {recall_pos:.4f}\")\n",
    "print(f\"F1(y=1)        = {f1_pos:.4f}\")\n",
    "\n",
    "# 7.4 — Ajuste de threshold usando probabilidade da classe 1\n",
    "# pega prob da classe 1: probability[1]\n",
    "\n",
    "pred_prob = (\n",
    "    predictions\n",
    "    .withColumn(\"prob_arr\", vector_to_array(F.col(\"probability\")))\n",
    "    .withColumn(\"p1\", F.col(\"prob_arr\")[1])\n",
    ")\n",
    "\n",
    "# thresholds simples\n",
    "thresholds = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5]\n",
    "\n",
    "print(\"\\n[Etapa 7.4] Varredura simples de threshold (classe 1):\")\n",
    "rows = []\n",
    "\n",
    "for t in thresholds:\n",
    "    tmp = pred_prob.withColumn(\"pred_t\", F.when(F.col(\"p1\") >= F.lit(t), 1).otherwise(0))\n",
    "\n",
    "    m = (\n",
    "        tmp.select(F.col(\"y\").cast(\"int\").alias(\"y\"), F.col(\"pred_t\").cast(\"int\").alias(\"pred\"))\n",
    "        .agg(\n",
    "            F.sum(F.when((F.col(\"y\")==1) & (F.col(\"pred\")==1), 1).otherwise(0)).alias(\"tp\"),\n",
    "            F.sum(F.when((F.col(\"y\")==0) & (F.col(\"pred\")==1), 1).otherwise(0)).alias(\"fp\"),\n",
    "            F.sum(F.when((F.col(\"y\")==1) & (F.col(\"pred\")==0), 1).otherwise(0)).alias(\"fn\"),\n",
    "        )\n",
    "        .collect()[0]\n",
    "    )\n",
    "\n",
    "    tp2, fp2, fn2 = m[\"tp\"], m[\"fp\"], m[\"fn\"]\n",
    "    p = tp2 / (tp2 + fp2) if (tp2 + fp2) else 0.0\n",
    "    r = tp2 / (tp2 + fn2) if (tp2 + fn2) else 0.0\n",
    "    f1 = (2*p*r/(p+r)) if (p+r) else 0.0\n",
    "    rows.append((t, tp2, fp2, fn2, p, r, f1))\n",
    "\n",
    "for (t, tp2, fp2, fn2, p, r, f1) in rows:\n",
    "    print(f\"thr={t:.2f} | TP={tp2} FP={fp2} FN={fn2} | Prec={p:.3f} Rec={r:.3f} F1={f1:.3f}\")\n",
    "\n",
    "# extra: mostra distribuição das probabilidades pra você entender escala\n",
    "print(\"\\n[Etapa 7.4] Distribuição de p1 (classe 1):\")\n",
    "pred_prob.select(\"p1\").summary(\"count\",\"min\",\"25%\",\"50%\",\"75%\",\"max\",\"mean\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f55a64f",
   "metadata": {
    "id": "2t-AWOFDD7kv"
   },
   "source": [
    "# MOMENTO 5 — Aplicação do modelo e geração de valor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb95f752",
   "metadata": {
    "id": "4nW0QDeefHFw"
   },
   "source": [
    "## ETAPA 8 — Aplicação prática do modelo no contexto de RH\n",
    "**Resumo:** demonstrar como o output do modelo pode ser utilizado para apoiar decisões reais de recrutamento e triagem de candidatos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddfd733",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WSEOqjUz29E8",
    "outputId": "d960793b-8b15-47c0-e315-cdf255ea04e0"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ETAPA 8 — MODELO BALANCEADO (UNIFICADO)\n",
    "# Objetivo: aumentar Recall da classe positiva (y=1)\n",
    "# Estratégia: class_weight manual\n",
    "# =========================\n",
    "\n",
    "\n",
    "# 8.1 — Calcula pesos por classe (baseado no treino)\n",
    "label_dist = (\n",
    "    train_df\n",
    "    .groupBy(\"y\")\n",
    "    .count()\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "counts = {row[\"y\"]: row[\"count\"] for row in label_dist}\n",
    "total = counts[0] + counts[1]\n",
    "\n",
    "w0 = total / (2 * counts[0])\n",
    "w1 = total / (2 * counts[1])\n",
    "\n",
    "print(f\"[Etapa 8.1] Pesos calculados: w0={w0:.3f} | w1={w1:.3f}\")\n",
    "\n",
    "# 8.2 — Cria coluna weight\n",
    "train_w = train_df.withColumn(\n",
    "    \"weight\",\n",
    "    F.when(F.col(\"y\") == 1, F.lit(w1)).otherwise(F.lit(w0))\n",
    ")\n",
    "\n",
    "test_w = test_df.withColumn(\"weight\", F.lit(1.0))\n",
    "\n",
    "# 8.3 — Pipeline completo (indexers + assembler + LR com weight)\n",
    "lr_bal = LogisticRegression(\n",
    "    labelCol=\"y\",\n",
    "    featuresCol=\"features\",\n",
    "    weightCol=\"weight\",\n",
    "    maxIter=50,\n",
    "    regParam=0.01\n",
    ")\n",
    "\n",
    "pipeline_bal = Pipeline(stages=indexers + [assembler, lr_bal])\n",
    "\n",
    "# 8.4 — Treino\n",
    "model_bal = pipeline_bal.fit(train_w)\n",
    "\n",
    "# 8.5 — Predição\n",
    "pred_bal = model_bal.transform(test_w)\n",
    "\n",
    "# 8.6 — AUC\n",
    "auc_eval = BinaryClassificationEvaluator(\n",
    "    labelCol=\"y\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "auc_bal = auc_eval.evaluate(pred_bal)\n",
    "print(f\"\\n[Etapa 8.6] AUC (balanceado) = {auc_bal:.4f}\")\n",
    "\n",
    "# 8.7 — Métricas da classe positiva (y=1)\n",
    "agg = (\n",
    "    pred_bal\n",
    "    .select(F.col(\"y\").cast(\"int\").alias(\"y\"), F.col(\"prediction\").cast(\"int\").alias(\"pred\"))\n",
    "    .agg(\n",
    "        F.sum(F.when((F.col(\"y\")==1) & (F.col(\"pred\")==1), 1).otherwise(0)).alias(\"tp\"),\n",
    "        F.sum(F.when((F.col(\"y\")==0) & (F.col(\"pred\")==1), 1).otherwise(0)).alias(\"fp\"),\n",
    "        F.sum(F.when((F.col(\"y\")==1) & (F.col(\"pred\")==0), 1).otherwise(0)).alias(\"fn\"),\n",
    "        F.sum(F.when((F.col(\"y\")==0) & (F.col(\"pred\")==0), 1).otherwise(0)).alias(\"tn\"),\n",
    "    )\n",
    ").collect()[0]\n",
    "\n",
    "TP, FP, FN, TN = agg[\"tp\"], agg[\"fp\"], agg[\"fn\"], agg[\"tn\"]\n",
    "\n",
    "precision_pos = TP / (TP + FP) if (TP + FP) else 0.0\n",
    "recall_pos    = TP / (TP + FN) if (TP + FN) else 0.0\n",
    "f1_pos        = (2 * precision_pos * recall_pos / (precision_pos + recall_pos)) if (precision_pos + recall_pos) else 0.0\n",
    "\n",
    "print(\"\\n[Etapa 8.7] Métricas classe positiva (y=1) — modelo balanceado:\")\n",
    "print(f\"TP={TP}  FP={FP}  FN={FN}  TN={TN}\")\n",
    "print(f\"Precision(y=1) = {precision_pos:.4f}\")\n",
    "print(f\"Recall(y=1)    = {recall_pos:.4f}\")\n",
    "print(f\"F1(y=1)        = {f1_pos:.4f}\")\n",
    "\n",
    "# 8.8 — Distribuição de probabilidade p1\n",
    "pred_bal_prob = (\n",
    "    pred_bal\n",
    "    .withColumn(\"p1\", vector_to_array(F.col(\"probability\"))[1])\n",
    ")\n",
    "\n",
    "print(\"\\n[Etapa 8.8] Distribuição p1 (classe 1) — modelo balanceado:\")\n",
    "pred_bal_prob.select(\"p1\").summary(\n",
    "    \"count\",\"min\",\"25%\",\"50%\",\"75%\",\"max\",\"mean\"\n",
    ").show(truncate=False)\n",
    "\n",
    "# 8.9 — (opcional) mostra 10 exemplos\n",
    "pred_bal_prob.select(\"y\", \"prediction\", \"p1\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d7ac9b",
   "metadata": {
    "id": "fgPRkgSBfI2Z"
   },
   "source": [
    "## ETAPA 9 — Construção do aplicativo analítico (Streamlit)\n",
    "**Resumo:** Disponibilizar os resultados do projeto em modelo interativo, buscando facilitar a interpretação e apoiar decisões estratégicas do RH\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255b7734",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RfPeH-D-A7Dr",
    "outputId": "ac3b2e25-7438-44a5-da03-31117e6d77de"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ETAPA 9 — THRESHOLD TUNING COM RECALL MÍNIMO (UNIFICADA E CORRIGIDA)\n",
    "# =========================\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 9.0 — GARANTIA DE INPUT\n",
    "# (corrige o erro: pred_bal não existia)\n",
    "# -------------------------------------------------\n",
    "pred_bal = model_bal.transform(test_df)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 9.1 — Extrai probabilidade da classe positiva (y=1)\n",
    "# -------------------------------------------------\n",
    "pred_thr = (\n",
    "    pred_bal\n",
    "    .select(\n",
    "        F.col(\"y\").cast(\"int\").alias(\"y\"),\n",
    "        vector_to_array(F.col(\"probability\"))[1].alias(\"p1\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 9.2 — Lista de thresholds (foco na região real do p1)\n",
    "# -------------------------------------------------\n",
    "thr_list = (\n",
    "    [round(i / 100, 2) for i in range(10, 51, 5)] +     # 0.10..0.50\n",
    "    [round(i / 1000, 3) for i in range(450, 551, 10)]  # 0.45..0.55\n",
    ")\n",
    "thr_list = sorted(set(thr_list))\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 9.3 — Função de métricas\n",
    "# -------------------------------------------------\n",
    "def metrics_for_threshold(df, thr):\n",
    "    tmp = df.withColumn(\"pred\", F.when(F.col(\"p1\") >= F.lit(thr), 1).otherwise(0))\n",
    "    r = tmp.agg(\n",
    "        F.sum(F.when((F.col(\"y\")==1) & (F.col(\"pred\")==1), 1).otherwise(0)).alias(\"tp\"),\n",
    "        F.sum(F.when((F.col(\"y\")==0) & (F.col(\"pred\")==1), 1).otherwise(0)).alias(\"fp\"),\n",
    "        F.sum(F.when((F.col(\"y\")==1) & (F.col(\"pred\")==0), 1).otherwise(0)).alias(\"fn\"),\n",
    "        F.sum(F.when((F.col(\"y\")==0) & (F.col(\"pred\")==0), 1).otherwise(0)).alias(\"tn\"),\n",
    "    ).collect()[0]\n",
    "\n",
    "    TP, FP, FN, TN = r[\"tp\"], r[\"fp\"], r[\"fn\"], r[\"tn\"]\n",
    "    precision = TP / (TP + FP) if (TP + FP) else 0.0\n",
    "    recall    = TP / (TP + FN) if (TP + FN) else 0.0\n",
    "    f1        = (2*precision*recall/(precision+recall)) if (precision+recall) else 0.0\n",
    "\n",
    "    return {\n",
    "        \"thr\": thr,\n",
    "        \"tp\": TP, \"fp\": FP, \"fn\": FN, \"tn\": TN,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 9.4 — Varredura completa\n",
    "# -------------------------------------------------\n",
    "results = [metrics_for_threshold(pred_thr, thr) for thr in thr_list]\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 9.5 — Seleção automática por recall mínimo\n",
    "# -------------------------------------------------\n",
    "targets = [0.80, 0.70, 0.60]\n",
    "\n",
    "def pick_best(rows, recall_min):\n",
    "    eligible = [r for r in rows if r[\"recall\"] >= recall_min]\n",
    "    if not eligible:\n",
    "        return None\n",
    "    eligible.sort(\n",
    "        key=lambda r: (r[\"precision\"], r[\"f1\"], r[\"thr\"]),\n",
    "        reverse=True\n",
    "    )\n",
    "    return eligible[0]\n",
    "\n",
    "best = None\n",
    "best_target = None\n",
    "for t in targets:\n",
    "    best = pick_best(results, t)\n",
    "    if best:\n",
    "        best_target = t\n",
    "        break\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 9.6 — OUTPUTS\n",
    "# -------------------------------------------------\n",
    "print(\"\\n[Etapa 9.1] Distribuição p1:\")\n",
    "pred_thr.select(\"p1\").summary(\"min\",\"25%\",\"50%\",\"75%\",\"max\",\"mean\").show(truncate=False)\n",
    "\n",
    "print(\"\\n[Etapa 9.2] Amostra de thresholds:\")\n",
    "for r in results:\n",
    "    if r[\"thr\"] in [0.10, 0.20, 0.30, 0.40, 0.45, 0.48, 0.50]:\n",
    "        print(f\"thr={r['thr']:.2f} | Prec={r['precision']:.3f} Rec={r['recall']:.3f} F1={r['f1']:.3f}\")\n",
    "\n",
    "if best is None:\n",
    "    print(\"\\n[Etapa 9.3] ❌ Nenhum threshold atingiu Recall >= 0.60\")\n",
    "else:\n",
    "    print(f\"\\n[Etapa 9.3] ✅ Melhor threshold com Recall >= {best_target:.2f}\")\n",
    "    print(f\"Threshold = {best['thr']}\")\n",
    "    print(f\"TP={best['tp']} FP={best['fp']} FN={best['fn']} TN={best['tn']}\")\n",
    "    print(f\"Precision = {best['precision']:.4f}\")\n",
    "    print(f\"Recall    = {best['recall']:.4f}\")\n",
    "    print(f\"F1        = {best['f1']:.4f}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 9.7 — Predição final com threshold escolhido\n",
    "# -------------------------------------------------\n",
    "if best:\n",
    "    pred_final = (\n",
    "        pred_bal\n",
    "        .withColumn(\"p1\", vector_to_array(F.col(\"probability\"))[1])\n",
    "        .withColumn(\"pred_final\", F.when(F.col(\"p1\") >= F.lit(best[\"thr\"]), 1).otherwise(0))\n",
    "    )\n",
    "\n",
    "    print(\"\\n[Etapa 9.7] Preview pred_final:\")\n",
    "    pred_final.select(\"y\", \"prediction\", \"p1\", \"pred_final\").show(10, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b3bee6",
   "metadata": {
    "id": "9Cs1cMFEEDxa"
   },
   "source": [
    "# MOMENTO 6 — Produto final, reflexão e encerramento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac922591",
   "metadata": {
    "id": "Mrz7EBq4rSRL"
   },
   "source": [
    "## ETAPA 10 — Análise crítica dos resultados e limitações do modelo\n",
    "**Resumo:** Discutir limitações técnicas, vieses dos dados e impactos dessas restrições na aplicação prática do modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce0efdf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UtmFlgCrQz6",
    "outputId": "f51b83e1-a045-45b1-a6b5-a7f1e807ac3f"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ETAPA 10 — Métricas por área (segmentação)\n",
    "# FIX: resolve NameError (DF não definido) + evita DIVIDE_BY_ZERO\n",
    "# =========================\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 10.0 — Resolver automaticamente qual dataframe usar\n",
    "# (ordem de preferência: pred_final -> pred_thr -> pred_prob -> predictions -> pred_bal)\n",
    "_candidates = [\"pred_final\", \"pred_thr\", \"pred_prob\", \"predictions\", \"pred_bal\", \"pred_final_df\"]\n",
    "df_seg = None\n",
    "for _name in _candidates:\n",
    "    if _name in globals() and globals()[_name] is not None:\n",
    "        df_seg = globals()[_name]\n",
    "        print(f\"[Etapa 10.0] Usando dataframe: {_name}\")\n",
    "        break\n",
    "\n",
    "if df_seg is None:\n",
    "    raise NameError(\n",
    "        \"Nenhum dataframe de predição encontrado. Esperado um destes nomes: \"\n",
    "        + \", \".join(_candidates)\n",
    "        + \". Crie/defina o DF da etapa 9 e rode a etapa 10 novamente.\"\n",
    "    )\n",
    "\n",
    "# 10.0.1 — Descobrir coluna de área (fallbacks)\n",
    "# você tem: app_area_atuacao (do output da etapa 10.1)\n",
    "area_col = None\n",
    "for c in [\"app_area_atuacao\", \"app_area_atuacao_norm\", \"area_atuacao\", \"app_area\", \"area\", \"segmento\"]:\n",
    "    if c in df_seg.columns:\n",
    "        area_col = c\n",
    "        break\n",
    "if area_col is None:\n",
    "    raise NameError(\n",
    "        \"Não encontrei coluna de área. Procurei por: app_area_atuacao, app_area_atuacao_norm, area_atuacao, app_area, area, segmento.\\n\"\n",
    "        f\"Colunas disponíveis: {df_seg.columns}\"\n",
    "    )\n",
    "\n",
    "# 10.0.2 — Garantir colunas y e pred_final\n",
    "# y deve existir (etapas anteriores). pred_final pode estar como pred_final ou prediction/prediction_final.\n",
    "y_col = \"y\" if \"y\" in df_seg.columns else None\n",
    "if y_col is None:\n",
    "    raise NameError(f\"Não encontrei coluna 'y'. Colunas disponíveis: {df_seg.columns}\")\n",
    "\n",
    "pred_col = None\n",
    "for c in [\"pred_final\", \"prediction_final\", \"pred\", \"prediction\"]:\n",
    "    if c in df_seg.columns:\n",
    "        pred_col = c\n",
    "        break\n",
    "if pred_col is None:\n",
    "    raise NameError(\n",
    "        \"Não encontrei coluna de predição final. Procurei por: pred_final, prediction_final, pred, prediction.\\n\"\n",
    "        f\"Colunas disponíveis: {df_seg.columns}\"\n",
    "    )\n",
    "\n",
    "print(f\"[Etapa 10.0] Colunas usadas: area='{area_col}' | y='{y_col}' | pred='{pred_col}'\")\n",
    "\n",
    "# 10.1 — Sanidade base segmentada\n",
    "print(\"\\n[Etapa 10.1] Sanidade base segmentada:\")\n",
    "(\n",
    "    df_seg\n",
    "    .groupBy(F.coalesce(F.col(area_col), F.lit(\"unknown\")).alias(area_col))\n",
    "    .count()\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "    .show(20, truncate=False)\n",
    ")\n",
    "\n",
    "# 10.2 — Métricas por área (TP/FP/FN/TN + Precision/Recall/F1) com divisão segura\n",
    "def safe_div(num_col, den_col):\n",
    "    return F.when(den_col != 0, (num_col / den_col)).otherwise(F.lit(None).cast(\"double\"))\n",
    "\n",
    "agg = (\n",
    "    df_seg\n",
    "    .select(\n",
    "        F.coalesce(F.col(area_col), F.lit(\"unknown\")).alias(\"area\"),\n",
    "        F.col(y_col).cast(\"int\").alias(\"y\"),\n",
    "        F.col(pred_col).cast(\"int\").alias(\"pred\")\n",
    "    )\n",
    "    .groupBy(\"area\")\n",
    "    .agg(\n",
    "        F.count(F.lit(1)).alias(\"n_total\"),\n",
    "        F.sum(F.when((F.col(\"y\")==1) & (F.col(\"pred\")==1), 1).otherwise(0)).alias(\"TP\"),\n",
    "        F.sum(F.when((F.col(\"y\")==0) & (F.col(\"pred\")==1), 1).otherwise(0)).alias(\"FP\"),\n",
    "        F.sum(F.when((F.col(\"y\")==1) & (F.col(\"pred\")==0), 1).otherwise(0)).alias(\"FN\"),\n",
    "        F.sum(F.when((F.col(\"y\")==0) & (F.col(\"pred\")==0), 1).otherwise(0)).alias(\"TN\"),\n",
    "        F.sum(F.when(F.col(\"y\")==1, 1).otherwise(0)).alias(\"n_y1\"),\n",
    "        F.sum(F.when(F.col(\"pred\")==1, 1).otherwise(0)).alias(\"n_pred1\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "metrics_by_area = (\n",
    "    agg\n",
    "    .withColumn(\"precision_y1\", safe_div(F.col(\"TP\"), (F.col(\"TP\")+F.col(\"FP\"))))\n",
    "    .withColumn(\"recall_y1\",    safe_div(F.col(\"TP\"), (F.col(\"TP\")+F.col(\"FN\"))))\n",
    "    .withColumn(\"f1_y1\", F.when(\n",
    "        (F.col(\"precision_y1\").isNotNull()) & (F.col(\"recall_y1\").isNotNull()) & ((F.col(\"precision_y1\")+F.col(\"recall_y1\")) != 0),\n",
    "        2*F.col(\"precision_y1\")*F.col(\"recall_y1\")/(F.col(\"precision_y1\")+F.col(\"recall_y1\"))\n",
    "    ).otherwise(F.lit(None).cast(\"double\")))\n",
    "    .withColumn(\"prevalencia_y1\", safe_div(F.col(\"n_y1\"), F.col(\"n_total\")))\n",
    "    .orderBy(F.desc(\"n_total\"))\n",
    ")\n",
    "\n",
    "print(\"\\n[Etapa 10.2] Métricas por área (ordenado por volume):\")\n",
    "metrics_by_area.select(\n",
    "    F.col(\"area\").alias(area_col),\n",
    "    \"n_total\",\"n_y1\",\"n_pred1\",\"TP\",\"FP\",\"FN\",\"TN\",\n",
    "    F.round(\"prevalencia_y1\",4).alias(\"prev_y1\"),\n",
    "    F.round(\"precision_y1\",4).alias(\"precision_y1\"),\n",
    "    F.round(\"recall_y1\",4).alias(\"recall_y1\"),\n",
    "    F.round(\"f1_y1\",4).alias(\"f1_y1\"),\n",
    ").show(30, truncate=False)\n",
    "\n",
    "# 10.3 — (Opcional) só áreas com volume mínimo\n",
    "print(\"\\n[Etapa 10.3] Métricas por área (apenas n_total >= 50):\")\n",
    "(\n",
    "    metrics_by_area\n",
    "    .where(F.col(\"n_total\") >= 50)\n",
    "    .select(\n",
    "        F.col(\"area\").alias(area_col),\n",
    "        \"n_total\",\"n_y1\",\n",
    "        F.round(\"precision_y1\",4).alias(\"precision_y1\"),\n",
    "        F.round(\"recall_y1\",4).alias(\"recall_y1\"),\n",
    "        F.round(\"f1_y1\",4).alias(\"f1_y1\"),\n",
    "    )\n",
    "    .show(30, truncate=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e020e33e",
   "metadata": {
    "id": "rxJuCVu--m-i"
   },
   "source": [
    "## ETAPA 11 — Propostas de melhoria e evolução do projeto\n",
    "**Resumo:** Apresentar caminhos futuros para aprimorar dados, modelos, métricas e integração com processos de RH.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866af60a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "acKkAFyy6um0",
    "outputId": "d9bddead-b03e-4f1a-8b23-297c3e312a53"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ETAPA 11 (OPÇÃO B) — FIX: cria p1 a partir de probability quando p1 não existir\n",
    "# =========================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 11.0 — INPUTS (ajuste só aqui)\n",
    "# -------------------------------------------------\n",
    "df_in = pred_bal  # <<< DF que tem: y, app_area_atuacao, probability (vector) e/ou p1\n",
    "area_col = \"app_area_atuacao\"\n",
    "y_col    = \"y\"\n",
    "\n",
    "# Se já tiver p1, use esse nome. Se não, extrairemos do vector probability.\n",
    "p1_col = \"p1\"\n",
    "prob_col = \"probability\"   # padrão do Spark ML\n",
    "\n",
    "# Alvo de recall por área (você pediu começar pelo mínimo)\n",
    "target_recall = 0.60\n",
    "\n",
    "# Regras de estabilidade\n",
    "min_n_total = 50\n",
    "min_pos     = 10\n",
    "\n",
    "# Fallback global (da sua etapa 9)\n",
    "global_thr = 0.46\n",
    "\n",
    "print(f\"[Etapa 11.0] DF usado = {df_in}\")\n",
    "print(f\"[Etapa 11.0] Config: target_recall={target_recall} | min_n_total={min_n_total} | min_pos={min_pos} | global_thr={global_thr}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 11.0b — Garante que existe coluna p1\n",
    "# -------------------------------------------------\n",
    "cols = set(df_in.columns)\n",
    "\n",
    "if p1_col in cols:\n",
    "    df_scored = df_in\n",
    "    print(\"[Etapa 11.0b] Coluna 'p1' já existe — ok.\")\n",
    "else:\n",
    "    if prob_col not in cols:\n",
    "        raise ValueError(f\"Não encontrei '{p1_col}' nem '{prob_col}' no df_in. Colunas disponíveis: {sorted(list(cols))[:40]} ...\")\n",
    "    df_scored = df_in.withColumn(p1_col, vector_to_array(F.col(prob_col))[1].cast(\"double\"))\n",
    "    print(\"[Etapa 11.0b] 'p1' não existia — criada a partir de probability[1].\")\n",
    "\n",
    "print(\"\\n[Etapa 11.0b] Preview (y, area, p1):\")\n",
    "df_scored.select(F.col(y_col).cast(\"int\").alias(\"y\"),\n",
    "                 F.col(area_col).cast(\"string\").alias(area_col),\n",
    "                 F.col(p1_col).cast(\"double\").alias(\"p1\")).show(5, truncate=False)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 11.1 — Base mínima (y, p1, área)\n",
    "# -------------------------------------------------\n",
    "df_base = (\n",
    "    df_scored\n",
    "    .select(\n",
    "        F.coalesce(F.col(area_col).cast(\"string\"), F.lit(\"unknown\")).alias(area_col),\n",
    "        F.col(y_col).cast(\"int\").alias(\"y\"),\n",
    "        F.col(p1_col).cast(\"double\").alias(\"p1\")\n",
    "    )\n",
    "    .fillna({area_col: \"unknown\"})\n",
    ")\n",
    "\n",
    "print(\"\\n[Etapa 11.1] Top áreas por volume:\")\n",
    "df_base.groupBy(area_col).count().orderBy(F.desc(\"count\")).show(20, truncate=False)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 11.2 — Threshold por área via quantil dos POSITIVOS\n",
    "# -------------------------------------------------\n",
    "q = float(1.0 - target_recall)  # recall 0.60 => quantil 0.40\n",
    "q = max(0.0, min(1.0, q))\n",
    "\n",
    "thr_by_area = (\n",
    "    df_base\n",
    "    .groupBy(area_col)\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"n_total\"),\n",
    "        F.sum(F.when(F.col(\"y\")==1, 1).otherwise(0)).alias(\"n_y1\"),\n",
    "        F.expr(f\"percentile_approx(CASE WHEN y=1 THEN p1 END, {q}, 10000)\").alias(\"thr_area_raw\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"use_area_thr\",\n",
    "        (F.col(\"n_total\") >= F.lit(min_n_total)) &\n",
    "        (F.col(\"n_y1\")   >= F.lit(min_pos)) &\n",
    "        F.col(\"thr_area_raw\").isNotNull()\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"thr_use\",\n",
    "        F.when(F.col(\"use_area_thr\"), F.col(\"thr_area_raw\")).otherwise(F.lit(global_thr))\n",
    "    )\n",
    "    .select(area_col, \"n_total\", \"n_y1\", \"use_area_thr\", \"thr_use\")\n",
    ")\n",
    "\n",
    "print(\"\\n[Etapa 11.2] Thresholds por área (fallback incluído):\")\n",
    "thr_by_area.orderBy(F.desc(\"n_total\")).show(50, truncate=False)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 11.3 — Aplica threshold por área e cria pred_final_B\n",
    "# -------------------------------------------------\n",
    "df_b = (\n",
    "    df_base\n",
    "    .join(thr_by_area.select(area_col, \"thr_use\", \"use_area_thr\"), on=area_col, how=\"left\")\n",
    "    .withColumn(\"pred_final_B\", F.when(F.col(\"p1\") >= F.col(\"thr_use\"), 1).otherwise(0))\n",
    ")\n",
    "\n",
    "print(\"\\n[Etapa 11.3] Preview:\")\n",
    "df_b.select(area_col, \"y\", \"p1\", \"thr_use\", \"use_area_thr\", \"pred_final_B\").show(10, truncate=False)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 11.4 — Métricas gerais (classe positiva)\n",
    "# -------------------------------------------------\n",
    "agg_all = df_b.agg(\n",
    "    F.sum(F.when((F.col(\"y\")==1) & (F.col(\"pred_final_B\")==1), 1).otherwise(0)).alias(\"TP\"),\n",
    "    F.sum(F.when((F.col(\"y\")==0) & (F.col(\"pred_final_B\")==1), 1).otherwise(0)).alias(\"FP\"),\n",
    "    F.sum(F.when((F.col(\"y\")==1) & (F.col(\"pred_final_B\")==0), 1).otherwise(0)).alias(\"FN\"),\n",
    "    F.sum(F.when((F.col(\"y\")==0) & (F.col(\"pred_final_B\")==0), 1).otherwise(0)).alias(\"TN\"),\n",
    ").collect()[0]\n",
    "\n",
    "TP, FP, FN, TN = int(agg_all[\"TP\"]), int(agg_all[\"FP\"]), int(agg_all[\"FN\"]), int(agg_all[\"TN\"])\n",
    "prec = TP / (TP + FP) if (TP + FP) > 0 else None\n",
    "rec  = TP / (TP + FN) if (TP + FN) > 0 else None\n",
    "f1   = (2*prec*rec)/(prec+rec) if (prec is not None and rec is not None and (prec+rec) > 0) else None\n",
    "\n",
    "print(\"\\n[Etapa 11.4] Métricas gerais — Opção B:\")\n",
    "print(f\"TP={TP} FP={FP} FN={FN} TN={TN}\")\n",
    "print(f\"Precision(y=1) = {prec:.4f}\" if prec is not None else \"Precision(y=1) = NULL\")\n",
    "print(f\"Recall(y=1)    = {rec:.4f}\"  if rec  is not None else \"Recall(y=1)    = NULL\")\n",
    "print(f\"F1(y=1)        = {f1:.4f}\"   if f1   is not None else \"F1(y=1)        = NULL\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 11.5 — Métricas por área (sem divisão por zero)\n",
    "# -------------------------------------------------\n",
    "metrics_area = (\n",
    "    df_b\n",
    "    .groupBy(area_col)\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"n_total\"),\n",
    "        F.sum(F.when(F.col(\"y\")==1, 1).otherwise(0)).alias(\"n_y1\"),\n",
    "        F.sum(F.when(F.col(\"pred_final_B\")==1, 1).otherwise(0)).alias(\"n_pred1\"),\n",
    "        F.sum(F.when((F.col(\"y\")==1) & (F.col(\"pred_final_B\")==1), 1).otherwise(0)).alias(\"TP\"),\n",
    "        F.sum(F.when((F.col(\"y\")==0) & (F.col(\"pred_final_B\")==1), 1).otherwise(0)).alias(\"FP\"),\n",
    "        F.sum(F.when((F.col(\"y\")==1) & (F.col(\"pred_final_B\")==0), 1).otherwise(0)).alias(\"FN\"),\n",
    "        F.sum(F.when((F.col(\"y\")==0) & (F.col(\"pred_final_B\")==0), 1).otherwise(0)).alias(\"TN\"),\n",
    "    )\n",
    "    .withColumn(\"prev_y1\", F.when(F.col(\"n_total\")>0, F.col(\"n_y1\")/F.col(\"n_total\")).otherwise(F.lit(None)))\n",
    "    .withColumn(\"precision_y1\", F.when((F.col(\"TP\")+F.col(\"FP\"))>0, F.col(\"TP\")/(F.col(\"TP\")+F.col(\"FP\"))).otherwise(F.lit(None)))\n",
    "    .withColumn(\"recall_y1\",    F.when((F.col(\"TP\")+F.col(\"FN\"))>0, F.col(\"TP\")/(F.col(\"TP\")+F.col(\"FN\"))).otherwise(F.lit(None)))\n",
    "    .withColumn(\n",
    "        \"f1_y1\",\n",
    "        F.when(\n",
    "            (F.col(\"precision_y1\").isNotNull()) & (F.col(\"recall_y1\").isNotNull()) & ((F.col(\"precision_y1\")+F.col(\"recall_y1\"))>0),\n",
    "            (2*F.col(\"precision_y1\")*F.col(\"recall_y1\"))/(F.col(\"precision_y1\")+F.col(\"recall_y1\"))\n",
    "        ).otherwise(F.lit(None))\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"\\n[Etapa 11.5] Métricas por área (n_total >= {min_n_total}):\")\n",
    "metrics_area.where(F.col(\"n_total\") >= F.lit(min_n_total)).orderBy(F.desc(\"n_total\")).select(\n",
    "    area_col, \"n_total\", \"n_y1\", \"precision_y1\", \"recall_y1\", \"f1_y1\"\n",
    ").show(50, truncate=False)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 11.6 — OUTPUTS\n",
    "# -------------------------------------------------\n",
    "pred_final_B = df_b\n",
    "threshold_table = thr_by_area\n",
    "\n",
    "print(\"\\n[Etapa 11] OUTPUTS prontos: pred_final_B / threshold_table\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d2e29d",
   "metadata": {
    "id": "LQWnXhHr-q1n"
   },
   "source": [
    "## ETAPA 12 — Considerações finais e aprendizados do projeto\n",
    "**Resumo:** Consolidar os principais aprendizados técnicos e analíticos obtidos ao longo do desenvolvimento do projeto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7155ea51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PWbE9Whe-tO8",
    "outputId": "645719fd-95a2-4bfa-fac1-10cfd9ec5f7b"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ETAPA 12 (UNIFICADA) — PR-curve + escolha de thresholds\n",
    "# Objetivo: manter recall >= target_recall e maximizar precision\n",
    "# Saídas: best_thr_global, threshold_table_best, pred_final_B2\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# ----------------------------\n",
    "# 12.0 — INPUTS (ajuste só aqui)\n",
    "# ----------------------------\n",
    "df_in = pred_bal  # <- ajuste se o seu DF tiver outro nome\n",
    "\n",
    "y_col = \"y\"\n",
    "area_col = \"app_area_atuacao\"\n",
    "prob_col = \"probability\"\n",
    "\n",
    "target_recall = 0.60\n",
    "min_n_total = 50\n",
    "min_pos = 10\n",
    "\n",
    "# grid de thresholds (0.00..1.00)\n",
    "thr_grid = [i/100 for i in range(0, 101)]\n",
    "thr_df = spark.createDataFrame([(t,) for t in thr_grid], [\"thr\"])\n",
    "\n",
    "print(f\"[Etapa 12.0] DF usado = {df_in}\")\n",
    "print(f\"[Etapa 12.0] Config: target_recall={target_recall} | min_n_total={min_n_total} | min_pos={min_pos}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 12.1 — Base de scoring (y, area, p1)\n",
    "# ----------------------------\n",
    "df_sc = (\n",
    "    df_in\n",
    "    .select(\n",
    "        F.col(y_col).cast(\"int\").alias(\"y\"),\n",
    "        F.coalesce(F.col(area_col).cast(\"string\"), F.lit(\"unknown\")).alias(area_col),\n",
    "        vector_to_array(F.col(prob_col))[1].cast(\"double\").alias(\"p1\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"[Etapa 12.1] Preview (y, area, p1):\")\n",
    "df_sc.select(\"y\", area_col, \"p1\").show(5, truncate=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 12.2 — PR GLOBAL: escolhe best_thr_global\n",
    "# Max precision mantendo recall >= target_recall\n",
    "# ----------------------------\n",
    "df_metrics_global = (\n",
    "    df_sc.select(\"y\", \"p1\").crossJoin(thr_df)\n",
    "    .withColumn(\"pred\", F.when(F.col(\"p1\") >= F.col(\"thr\"), 1).otherwise(0))\n",
    "    .groupBy(\"thr\")\n",
    "    .agg(\n",
    "        F.sum(F.when((F.col(\"y\")==1) & (F.col(\"pred\")==1), 1).otherwise(0)).alias(\"TP\"),\n",
    "        F.sum(F.when((F.col(\"y\")==0) & (F.col(\"pred\")==1), 1).otherwise(0)).alias(\"FP\"),\n",
    "        F.sum(F.when((F.col(\"y\")==1) & (F.col(\"pred\")==0), 1).otherwise(0)).alias(\"FN\"),\n",
    "        F.sum(F.when((F.col(\"y\")==0) & (F.col(\"pred\")==0), 1).otherwise(0)).alias(\"TN\"),\n",
    "    )\n",
    "    .withColumn(\"precision\", F.when((F.col(\"TP\")+F.col(\"FP\"))>0, F.col(\"TP\")/(F.col(\"TP\")+F.col(\"FP\"))))\n",
    "    .withColumn(\"recall\",    F.when((F.col(\"TP\")+F.col(\"FN\"))>0, F.col(\"TP\")/(F.col(\"TP\")+F.col(\"FN\"))))\n",
    "    .withColumn(\n",
    "        \"f1\",\n",
    "        F.when(\n",
    "            (F.col(\"precision\").isNotNull()) & (F.col(\"recall\").isNotNull()) & ((F.col(\"precision\")+F.col(\"recall\"))>0),\n",
    "            (2*F.col(\"precision\")*F.col(\"recall\"))/(F.col(\"precision\")+F.col(\"recall\"))\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"[Etapa 12.2] Top thresholds globais (recall >= alvo) ordenado por precision:\")\n",
    "df_best_global = (\n",
    "    df_metrics_global\n",
    "    .where(F.col(\"recall\") >= F.lit(target_recall))\n",
    "    .orderBy(F.desc(\"precision\"), F.desc(\"f1\"), F.desc(\"thr\"))\n",
    ")\n",
    "df_best_global.show(20, truncate=False)\n",
    "\n",
    "best_row = df_best_global.first()\n",
    "best_thr_global = float(best_row[\"thr\"]) if best_row else None\n",
    "print(f\"[Etapa 12.2] BEST global_thr (max precision com recall>={target_recall}): {best_thr_global}\")\n",
    "\n",
    "# fallback se por algum motivo não existir linha (muito raro)\n",
    "if best_thr_global is None:\n",
    "    best_thr_global = 0.50\n",
    "    print(f\"[Etapa 12.2] WARNING: fallback best_thr_global={best_thr_global}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 12.3 — Áreas elegíveis (volume e positivos)\n",
    "# ----------------------------\n",
    "area_stats = (\n",
    "    df_sc.groupBy(area_col)\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"n_total\"),\n",
    "        F.sum(F.when(F.col(\"y\")==1, 1).otherwise(0)).alias(\"n_y1\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"[Etapa 12.3] Top áreas por volume:\")\n",
    "area_stats.orderBy(F.desc(\"n_total\")).show(20, truncate=False)\n",
    "\n",
    "area_eligible = area_stats.where((F.col(\"n_total\")>=min_n_total) & (F.col(\"n_y1\")>=min_pos)).select(area_col)\n",
    "\n",
    "# ----------------------------\n",
    "# 12.4 — PR POR ÁREA: escolhe thr_area_best\n",
    "# Max precision mantendo recall >= target_recall (por área)\n",
    "# ----------------------------\n",
    "df_m_area = (\n",
    "    df_sc.join(area_eligible, on=area_col, how=\"inner\")\n",
    "    .crossJoin(thr_df)\n",
    "    .withColumn(\"pred\", F.when(F.col(\"p1\") >= F.col(\"thr\"), 1).otherwise(0))\n",
    "    .groupBy(area_col, \"thr\")\n",
    "    .agg(\n",
    "        F.sum(F.when((F.col(\"y\")==1) & (F.col(\"pred\")==1), 1).otherwise(0)).alias(\"TP\"),\n",
    "        F.sum(F.when((F.col(\"y\")==0) & (F.col(\"pred\")==1), 1).otherwise(0)).alias(\"FP\"),\n",
    "        F.sum(F.when((F.col(\"y\")==1) & (F.col(\"pred\")==0), 1).otherwise(0)).alias(\"FN\"),\n",
    "    )\n",
    "    .withColumn(\"precision\", F.when((F.col(\"TP\")+F.col(\"FP\"))>0, F.col(\"TP\")/(F.col(\"TP\")+F.col(\"FP\"))))\n",
    "    .withColumn(\"recall\",    F.when((F.col(\"TP\")+F.col(\"FN\"))>0, F.col(\"TP\")/(F.col(\"TP\")+F.col(\"FN\"))))\n",
    "    .withColumn(\n",
    "        \"f1\",\n",
    "        F.when(\n",
    "            (F.col(\"precision\").isNotNull()) & (F.col(\"recall\").isNotNull()) & ((F.col(\"precision\")+F.col(\"recall\"))>0),\n",
    "            (2*F.col(\"precision\")*F.col(\"recall\"))/(F.col(\"precision\")+F.col(\"recall\"))\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(area_col).orderBy(F.desc(\"precision\"), F.desc(\"f1\"), F.desc(\"thr\"))\n",
    "\n",
    "threshold_table_best = (\n",
    "    df_m_area\n",
    "    .where(F.col(\"recall\") >= F.lit(target_recall))\n",
    "    .withColumn(\"rn\", F.row_number().over(w))\n",
    "    .where(F.col(\"rn\")==1)\n",
    "    .select(\n",
    "        area_col,\n",
    "        F.col(\"thr\").alias(\"thr_area_best\"),\n",
    "        \"precision\",\n",
    "        \"recall\",\n",
    "        \"f1\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"[Etapa 12.4] Thresholds BEST por área (max precision com recall >= alvo):\")\n",
    "threshold_table_best.orderBy(F.desc(\"precision\")).show(50, truncate=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 12.5 — Aplica threshold por área + fallback global\n",
    "# ----------------------------\n",
    "pred_final_B2 = (\n",
    "    df_sc\n",
    "    .join(threshold_table_best.select(area_col, \"thr_area_best\"), on=area_col, how=\"left\")\n",
    "    .withColumn(\"thr_use\", F.coalesce(F.col(\"thr_area_best\"), F.lit(best_thr_global)))\n",
    "    .withColumn(\"pred_final_B2\", F.when(F.col(\"p1\") >= F.col(\"thr_use\"), 1).otherwise(0))\n",
    ")\n",
    "\n",
    "print(\"[Etapa 12.5] Preview (area, y, p1, thr_use, pred_final_B2):\")\n",
    "pred_final_B2.select(area_col, \"y\", \"p1\", \"thr_use\", \"pred_final_B2\").show(10, truncate=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 12.6 — Métricas gerais (B2)\n",
    "# ----------------------------\n",
    "m_global = (\n",
    "    pred_final_B2\n",
    "    .agg(\n",
    "        F.sum(F.when((F.col(\"y\")==1) & (F.col(\"pred_final_B2\")==1), 1).otherwise(0)).alias(\"TP\"),\n",
    "        F.sum(F.when((F.col(\"y\")==0) & (F.col(\"pred_final_B2\")==1), 1).otherwise(0)).alias(\"FP\"),\n",
    "        F.sum(F.when((F.col(\"y\")==1) & (F.col(\"pred_final_B2\")==0), 1).otherwise(0)).alias(\"FN\"),\n",
    "        F.sum(F.when((F.col(\"y\")==0) & (F.col(\"pred_final_B2\")==0), 1).otherwise(0)).alias(\"TN\"),\n",
    "    )\n",
    "    .collect()[0]\n",
    ")\n",
    "\n",
    "TP, FP, FN, TN = int(m_global[\"TP\"]), int(m_global[\"FP\"]), int(m_global[\"FN\"]), int(m_global[\"TN\"])\n",
    "precision = TP/(TP+FP) if (TP+FP)>0 else None\n",
    "recall    = TP/(TP+FN) if (TP+FN)>0 else None\n",
    "f1        = (2*precision*recall)/(precision+recall) if (precision is not None and recall is not None and (precision+recall)>0) else None\n",
    "\n",
    "print(\"[Etapa 12.6] Métricas gerais — Opção B2 (PR-otimizada):\")\n",
    "print(f\"TP={TP} FP={FP} FN={FN} TN={TN}\")\n",
    "print(f\"Precision(y=1) = {precision}\")\n",
    "print(f\"Recall(y=1)    = {recall}\")\n",
    "print(f\"F1(y=1)        = {f1}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 12.7 — Métricas por área (n_total >= min_n_total)\n",
    "# ----------------------------\n",
    "df_area_metrics = (\n",
    "    pred_final_B2\n",
    "    .groupBy(area_col)\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"n_total\"),\n",
    "        F.sum(F.when(F.col(\"y\")==1, 1).otherwise(0)).alias(\"n_y1\"),\n",
    "        F.sum(F.when((F.col(\"y\")==1) & (F.col(\"pred_final_B2\")==1), 1).otherwise(0)).alias(\"TP\"),\n",
    "        F.sum(F.when((F.col(\"y\")==0) & (F.col(\"pred_final_B2\")==1), 1).otherwise(0)).alias(\"FP\"),\n",
    "        F.sum(F.when((F.col(\"y\")==1) & (F.col(\"pred_final_B2\")==0), 1).otherwise(0)).alias(\"FN\"),\n",
    "    )\n",
    "    .withColumn(\"precision_y1\", F.when((F.col(\"TP\")+F.col(\"FP\"))>0, F.col(\"TP\")/(F.col(\"TP\")+F.col(\"FP\"))))\n",
    "    .withColumn(\"recall_y1\",    F.when((F.col(\"TP\")+F.col(\"FN\"))>0, F.col(\"TP\")/(F.col(\"TP\")+F.col(\"FN\"))))\n",
    "    .withColumn(\n",
    "        \"f1_y1\",\n",
    "        F.when(\n",
    "            (F.col(\"precision_y1\").isNotNull()) & (F.col(\"recall_y1\").isNotNull()) & ((F.col(\"precision_y1\")+F.col(\"recall_y1\"))>0),\n",
    "            (2*F.col(\"precision_y1\")*F.col(\"recall_y1\"))/(F.col(\"precision_y1\")+F.col(\"recall_y1\"))\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"[Etapa 12.7] Métricas por área (n_total >= {min_n_total}):\")\n",
    "df_area_metrics.where(F.col(\"n_total\")>=F.lit(min_n_total)).orderBy(F.desc(\"n_total\")).select(\n",
    "    area_col, \"n_total\", \"n_y1\", \"precision_y1\", \"recall_y1\", \"f1_y1\"\n",
    ").show(50, truncate=False)\n",
    "\n",
    "print(\"[Etapa 12] OUTPUTS prontos: pred_final_B2 / threshold_table_best / best_thr_global\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74739232",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ejFCWvElhXsc",
    "outputId": "47011862-7385-4565-a0e6-37deebfce7f8"
   },
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# ETAPA 12.8 — Exportar CSV final para Streamlit\n",
    "# =========================================\n",
    "\n",
    "# Escolha do DF final\n",
    "df_final = pred_final_B2   # ou pred_final_B, se preferir\n",
    "\n",
    "# Colunas mínimas que o app usa\n",
    "cols_export = [\n",
    "    \"app_area_atuacao\",\n",
    "    \"p1\",\n",
    "    \"pred_final_B2\"\n",
    "]\n",
    "\n",
    "# Inclui y se existir (para métricas no app)\n",
    "if \"y\" in df_final.columns:\n",
    "    cols_export.append(\"y\")\n",
    "\n",
    "df_export = df_final.select(*cols_export)\n",
    "\n",
    "# Converter para Pandas (Streamlit trabalha com CSV/Pandas)\n",
    "pdf = df_export.toPandas()\n",
    "\n",
    "# Caminho padrão usado pelo app\n",
    "OUTPUT_PATH = \"/content/predicoes_streamlit.csv\"\n",
    "pdf.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(\"✅ CSV para o Streamlit gerado com sucesso!\")\n",
    "print(\"📁 Caminho:\", OUTPUT_PATH)\n",
    "print(\"🔎 Preview:\")\n",
    "print(pdf.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca11cef8",
   "metadata": {
    "id": "_Nes82G9UqXB"
   },
   "source": [
    "## ETAPA 13 — Setup e execução do aplicativo Streamlit\n",
    "**Resumo:** Configurar ambiente, estruturar o app e disponibilizar a aplicação interativa como entrega final do projeto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaa89e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sRQxdYR1UnKK",
    "outputId": "b50e2c52-c3ef-43f0-c294-dced705a52d9"
   },
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# ETAPA 13 (UNIFICADA) — Streamlit no Google Colab (FIX cloudflared)\n",
    "# =========================================\n",
    "\n",
    "# 1) Instalar Streamlit (cloudflared via binário oficial, NÃO via pip)\n",
    "!pip -q install streamlit pandas numpy\n",
    "\n",
    "import os, json, time, re, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "APP_DIR = Path(\"/content/streamlit_app\")\n",
    "APP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2) Baixar binário oficial do cloudflared (Linux x86_64 — padrão do Colab)\n",
    "CLOUDFLARED_PATH = Path(\"/content/cloudflared\")\n",
    "if not CLOUDFLARED_PATH.exists():\n",
    "    !wget -q -O /content/cloudflared https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
    "    !chmod +x /content/cloudflared\n",
    "\n",
    "# 3) (Opcional) Se seus arquivos estiverem em outro caminho, ajuste aqui:\n",
    "THRESHOLD_CSV_DEFAULT = \"/content/threshold_table_best.csv\"\n",
    "GLOBAL_JSON_DEFAULT   = \"/content/best_thr_global.json\"\n",
    "\n",
    "# 4) Escrever o app Streamlit\n",
    "app_code = rf'''\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import re\n",
    "\n",
    "st.set_page_config(page_title=\"Scoring de Candidatos — Threshold por Área\", layout=\"wide\")\n",
    "st.title(\"📌 Scoring de Candidatos — Threshold por Área (fallback global)\")\n",
    "\n",
    "# --------------------------------\n",
    "# Helpers\n",
    "# --------------------------------\n",
    "def load_best_thr(path_json: str, default_thr: float = 0.48) -> float:\n",
    "    try:\n",
    "        with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        if isinstance(data, dict):\n",
    "            for k in [\"best_thr_global\", \"global_thr\", \"thr\", \"threshold\"]:\n",
    "                if k in data:\n",
    "                    return float(data[k])\n",
    "            return float(list(data.values())[0])\n",
    "        return float(data)\n",
    "    except Exception:\n",
    "        return float(default_thr)\n",
    "\n",
    "def load_threshold_table(path_csv: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path_csv)\n",
    "    expected = {{\"app_area_atuacao\", \"thr_area_best\"}}\n",
    "    if not expected.issubset(set(df.columns)):\n",
    "        raise ValueError(\n",
    "            f\"threshold_table_best.csv precisa ter as colunas {{sorted(list(expected))}}. \"\n",
    "            f\"Encontradas: {{df.columns.tolist()}}\"\n",
    "        )\n",
    "    df[\"app_area_atuacao\"] = df[\"app_area_atuacao\"].astype(str).fillna(\"unknown\")\n",
    "    df[\"thr_area_best\"] = pd.to_numeric(df[\"thr_area_best\"], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def ensure_p1(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if \"p1\" in df.columns:\n",
    "        df[\"p1\"] = pd.to_numeric(df[\"p1\"], errors=\"coerce\")\n",
    "        return df\n",
    "\n",
    "    if \"probability\" in df.columns:\n",
    "        def parse_prob(x):\n",
    "            if pd.isna(x):\n",
    "                return np.nan\n",
    "            if isinstance(x, (list, tuple, np.ndarray)) and len(x) >= 2:\n",
    "                return float(x[1])\n",
    "            s = str(x).strip()\n",
    "            nums = re.findall(r\"[-+]?\\d*\\.\\d+|[-+]?\\d+\", s)\n",
    "            if len(nums) >= 2:\n",
    "                return float(nums[1])\n",
    "            return np.nan\n",
    "\n",
    "        df[\"p1\"] = df[\"probability\"].apply(parse_prob)\n",
    "        return df\n",
    "\n",
    "    return df\n",
    "\n",
    "def apply_thresholds(df: pd.DataFrame, thr_table: pd.DataFrame, global_thr: float) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"app_area_atuacao\"] = out.get(\"app_area_atuacao\", \"unknown\").astype(str).fillna(\"unknown\")\n",
    "\n",
    "    thr = thr_table[[\"app_area_atuacao\",\"thr_area_best\"]].copy()\n",
    "    out = out.merge(thr, on=\"app_area_atuacao\", how=\"left\")\n",
    "\n",
    "    out[\"thr_use\"] = out[\"thr_area_best\"].fillna(global_thr).astype(float)\n",
    "    out[\"use_area_thr\"] = out[\"thr_area_best\"].notna()\n",
    "\n",
    "    if \"p1\" not in out.columns:\n",
    "        raise ValueError(\"Não encontrei 'p1'. Envie CSV com 'p1' OU 'probability' parseável.\")\n",
    "\n",
    "    out[\"p1\"] = pd.to_numeric(out[\"p1\"], errors=\"coerce\")\n",
    "    if out[\"p1\"].isna().all():\n",
    "        raise ValueError(\"'p1' existe, mas está toda nula/NaN. Verifique seu arquivo.\")\n",
    "\n",
    "    out[\"pred_final\"] = (out[\"p1\"] >= out[\"thr_use\"]).astype(int)\n",
    "    return out\n",
    "\n",
    "def metrics_if_y_exists(df: pd.DataFrame) -> dict:\n",
    "    if \"y\" not in df.columns:\n",
    "        return {{}}\n",
    "    y = pd.to_numeric(df[\"y\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    pred = pd.to_numeric(df[\"pred_final\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    tp = int(((y==1) & (pred==1)).sum())\n",
    "    fp = int(((y==0) & (pred==1)).sum())\n",
    "    fn = int(((y==1) & (pred==0)).sum())\n",
    "    tn = int(((y==0) & (pred==0)).sum())\n",
    "\n",
    "    prec = tp / (tp + fp) if (tp+fp) else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp+fn) else 0.0\n",
    "    f1   = (2*prec*rec)/(prec+rec) if (prec+rec) else 0.0\n",
    "\n",
    "    return {{\"TP\":tp,\"FP\":fp,\"FN\":fn,\"TN\":tn,\"precision\":prec,\"recall\":rec,\"f1\":f1}}\n",
    "\n",
    "def to_csv_bytes(df: pd.DataFrame) -> bytes:\n",
    "    return df.to_csv(index=False).encode(\"utf-8\")\n",
    "\n",
    "# --------------------------------\n",
    "# Sidebar config\n",
    "# --------------------------------\n",
    "with st.sidebar:\n",
    "    st.header(\"Config\")\n",
    "    threshold_csv = st.text_input(\"Caminho do threshold_table_best.csv\", value=\"{THRESHOLD_CSV_DEFAULT}\")\n",
    "    global_json   = st.text_input(\"Caminho do best_thr_global.json\", value=\"{GLOBAL_JSON_DEFAULT}\")\n",
    "    default_thr   = st.number_input(\"Fallback global (se JSON falhar)\", value=0.48, min_value=0.0, max_value=1.0, step=0.01)\n",
    "\n",
    "# Carrega thresholds\n",
    "try:\n",
    "    thr_table = load_threshold_table(threshold_csv)\n",
    "    global_thr = load_best_thr(global_json, default_thr=float(default_thr))\n",
    "    st.success(f\"Thresholds carregados ✅ | global_thr={{global_thr:.2f}} | áreas={{thr_table.shape[0]}}\")\n",
    "except Exception as e:\n",
    "    st.error(f\"Falha ao carregar thresholds: {{e}}\")\n",
    "    st.stop()\n",
    "\n",
    "st.caption(\"Envie um CSV com pelo menos: **app_area_atuacao** e **p1** (ou **probability** parseável). Se tiver **y**, calcula métricas.\")\n",
    "\n",
    "uploaded = st.file_uploader(\"📤 Upload do CSV de candidatos\", type=[\"csv\"])\n",
    "if uploaded is None:\n",
    "    st.info(\"Faça upload de um CSV para começar.\")\n",
    "    st.stop()\n",
    "\n",
    "# Lê CSV\n",
    "try:\n",
    "    df_raw = pd.read_csv(uploaded)\n",
    "except Exception as e:\n",
    "    st.error(f\"Não consegui ler seu CSV: {{e}}\")\n",
    "    st.stop()\n",
    "\n",
    "# Valida mínima\n",
    "if \"app_area_atuacao\" not in df_raw.columns:\n",
    "    st.error(\"Coluna obrigatória ausente: **app_area_atuacao**\")\n",
    "    st.write(\"Colunas encontradas:\", df_raw.columns.tolist())\n",
    "    st.stop()\n",
    "\n",
    "df_raw = ensure_p1(df_raw)\n",
    "\n",
    "# Aplica thresholds\n",
    "try:\n",
    "    df_scored = apply_thresholds(df_raw, thr_table, global_thr)\n",
    "except Exception as e:\n",
    "    st.error(f\"Erro ao aplicar thresholds: {{e}}\")\n",
    "    st.write(\"Colunas encontradas:\", df_raw.columns.tolist())\n",
    "    st.stop()\n",
    "\n",
    "# KPIs\n",
    "c1, c2, c3, c4 = st.columns(4)\n",
    "c1.metric(\"Registros\", f\"{{len(df_scored):,}}\".replace(\",\", \".\"))\n",
    "c2.metric(\"Pred=1\", f\"{{int(df_scored['pred_final'].sum()):,}}\".replace(\",\", \".\"))\n",
    "c3.metric(\"Global thr\", f\"{{global_thr:.2f}}\")\n",
    "c4.metric(\"Áreas no CSV\", f\"{{df_scored['app_area_atuacao'].nunique():,}}\".replace(\",\", \".\"))\n",
    "\n",
    "m = metrics_if_y_exists(df_scored)\n",
    "if m:\n",
    "    st.subheader(\"📈 Métricas (seu CSV tem coluna y)\")\n",
    "    mc1, mc2, mc3, mc4 = st.columns(4)\n",
    "    mc1.metric(\"Precision\", f\"{{m['precision']:.4f}}\")\n",
    "    mc2.metric(\"Recall\", f\"{{m['recall']:.4f}}\")\n",
    "    mc3.metric(\"F1\", f\"{{m['f1']:.4f}}\")\n",
    "    mc4.metric(\"TP / FP / FN / TN\", f\"{{m['TP']}} / {{m['FP']}} / {{m['FN']}} / {{m['TN']}}\")\n",
    "\n",
    "st.subheader(\"🔎 Preview\")\n",
    "st.dataframe(df_scored.head(50), use_container_width=True)\n",
    "\n",
    "st.subheader(\"⬇️ Download\")\n",
    "st.download_button(\n",
    "    \"Baixar CSV com pred_final (scored.csv)\",\n",
    "    data=to_csv_bytes(df_scored),\n",
    "    file_name=\"scored.csv\",\n",
    "    mime=\"text/csv\"\n",
    ")\n",
    "\n",
    "with st.expander(\"Ver thresholds por área (tabela carregada)\"):\n",
    "    st.dataframe(thr_table.sort_values(\"app_area_atuacao\").reset_index(drop=True), use_container_width=True)\n",
    "'''\n",
    "\n",
    "(APP_DIR / \"app.py\").write_text(app_code, encoding=\"utf-8\")\n",
    "print(\"✅ App escrito em:\", APP_DIR / \"app.py\")\n",
    "\n",
    "# 5) Limpar processos antigos (se existirem)\n",
    "!pkill -f \"streamlit run\" -9 || true\n",
    "!pkill -f \"cloudflared tunnel\" -9 || true\n",
    "\n",
    "# 6) Subir Streamlit\n",
    "p_streamlit = subprocess.Popen(\n",
    "    [\"streamlit\", \"run\", str(APP_DIR / \"app.py\"), \"--server.port\", \"8501\", \"--server.address\", \"0.0.0.0\"],\n",
    "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n",
    ")\n",
    "\n",
    "# 7) Abrir tunnel com o BINÁRIO /content/cloudflared\n",
    "p_tunnel = subprocess.Popen(\n",
    "    [str(CLOUDFLARED_PATH), \"tunnel\", \"--url\", \"http://localhost:8501\"],\n",
    "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n",
    ")\n",
    "\n",
    "# 8) Capturar URL pública\n",
    "public_url = None\n",
    "start = time.time()\n",
    "while time.time() - start < 60:\n",
    "    line = p_tunnel.stdout.readline()\n",
    "    if not line:\n",
    "        continue\n",
    "    m = re.search(r\"(https://[a-zA-Z0-9\\-]+\\.trycloudflare\\.com)\", line)\n",
    "    if m:\n",
    "        public_url = m.group(1)\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if public_url:\n",
    "    print(\"✅ Streamlit no Colab rodando! Acesse a URL:\")\n",
    "    print(public_url)\n",
    "else:\n",
    "    print(\"⚠️ Não consegui capturar a URL automaticamente.\")\n",
    "    print(\"➡️ Veja as linhas acima do cloudflared e procure um link .trycloudflare.com\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 9) Mostrar alguns logs iniciais do Streamlit (pra garantir que subiu)\n",
    "print(\"\\n--- LOGS Streamlit (primeiras linhas) ---\")\n",
    "for _ in range(20):\n",
    "    try:\n",
    "        print(p_streamlit.stdout.readline().rstrip())\n",
    "    except:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2dde9e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f9902bf549e44d499ad3f535892da83c",
      "3a77885c7d384c8995eecf1eed98cc86",
      "a7e809c7886b473782e39fbfe8899eb7",
      "abff7d4b16464143b3b640f583ad0c2c",
      "1501cad4a1cd43f98db50fd6861d42d7",
      "65faf6dce051424db1497bc441a593dd",
      "681a3f8a79a14b52b5e67ba57b0137b4",
      "b73591c6d2e5474ca2d87f46b6e23080",
      "5c792c097a5a4888a08312efd11ee757",
      "8af9e79b155140edafa2d4071021ec01",
      "9f907e13bc7540b092a75d84a3ad6abe",
      "f9ae734f6d284ad9974ffba121240606",
      "44f0341dc2b841bfa86c39718468d2e8",
      "9f3642989e82481b8b5c6965a7026131",
      "70ef8461f9114c1d80fe3b779ba7e8a5",
      "42213bce1cf64068a12f5ceed2d65789",
      "27b997232aba402a8da7e776fc22868d",
      "0f73dd56819b42d5bc8c9a594dfe7940",
      "72018f696db54852b089863bf78fde62",
      "bf2411ec290f4ae28b49ea5f84b38557",
      "ef5a00e265d848168bb42458d19f4dbf",
      "2834e76bcd0b4cd8a1d8987ee24efee0"
     ]
    },
    "id": "mbW_dIyLmQFk",
    "outputId": "212c2d60-ff25-4532-d6ea-54af36ac1339"
   },
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# ETAPA 16.1 — FIX: textos vazios => sim_score_bert tudo 0\n",
    "# Diagnóstico + seleção automática de colunas textuais + recompute embeddings\n",
    "# =========================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# -------------------------\n",
    "# Configs\n",
    "# -------------------------\n",
    "MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "CACHE_DIR = \"/content/nlp_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "APP_EMB_PATH = os.path.join(CACHE_DIR, \"app_embeddings.parquet\")\n",
    "JOB_EMB_PATH = os.path.join(CACHE_DIR, \"job_embeddings.parquet\")\n",
    "OUT_SCORED_PATH = os.path.join(CACHE_DIR, \"df_scored_with_bert.parquet\")\n",
    "\n",
    "print(f\"[Etapa 16.1] DEVICE={DEVICE} | MODEL={MODEL_NAME} | CACHE={CACHE_DIR}\")\n",
    "\n",
    "# -------------------------\n",
    "# Inputs obrigatórios\n",
    "# -------------------------\n",
    "df_apps = globals().get(\"df_apps\", None)\n",
    "df_vagas = globals().get(\"df_vagas\", None)\n",
    "\n",
    "if df_apps is None or df_vagas is None:\n",
    "    raise ValueError(\"Não encontrei df_apps e/ou df_vagas no ambiente. Rode as etapas anteriores que criam esses DataFrames.\")\n",
    "\n",
    "# -------------------------\n",
    "# 16.1a — Join robusto (mesmo que só exista __root_id)\n",
    "# -------------------------\n",
    "PREFERRED_JOIN_CANDIDATES = [\n",
    "    \"vaga_id\",\"job_id\",\"id_vaga\",\"position_id\",\"id_position\",\n",
    "    \"codigo_vaga\",\"cod_vaga\",\"id_job\",\"jobid\",\"vacancy_id\",\"__root_id\"\n",
    "]\n",
    "\n",
    "def detect_join_key(df_left, df_right, preferred):\n",
    "    common = sorted(list(set(df_left.columns).intersection(set(df_right.columns))))\n",
    "    print(f\"\\n[Etapa 16.1] Colunas em comum ({len(common)}): {common[:200]}\")\n",
    "    for c in preferred:\n",
    "        if c in common:\n",
    "            return c\n",
    "    if len(common) == 1:\n",
    "        return common[0]\n",
    "    return None\n",
    "\n",
    "JOIN_KEY = detect_join_key(df_apps, df_vagas, PREFERRED_JOIN_CANDIDATES)\n",
    "if JOIN_KEY is None:\n",
    "    raise ValueError(\"Não encontrei coluna de join entre df_apps e df_vagas. Você precisa definir uma chave manualmente.\")\n",
    "\n",
    "print(f\"[Etapa 16.1] JOIN_KEY='{JOIN_KEY}'\")\n",
    "df_join = df_apps.join(df_vagas, on=JOIN_KEY, how=\"inner\")\n",
    "\n",
    "# cria ids se não existirem\n",
    "if \"app_id\" not in df_join.columns:\n",
    "    df_join = df_join.withColumn(\"app_id\", F.monotonically_increasing_id())\n",
    "if \"vaga_id\" not in df_join.columns:\n",
    "    df_join = df_join.withColumn(\"vaga_id\", F.monotonically_increasing_id())\n",
    "\n",
    "# -------------------------\n",
    "# 16.1b — Auto-descoberta de colunas textuais úteis\n",
    "# -------------------------\n",
    "# Ideia: entre colunas string, calcular média do length (ignorando vazios) e escolher as top N.\n",
    "\n",
    "def top_text_columns(df, exclude_cols=set(), topn=6, sample_frac=0.15, seed=42):\n",
    "    string_cols = [c for c, t in df.dtypes if t == \"string\" and c not in exclude_cols]\n",
    "    if not string_cols:\n",
    "        return []\n",
    "\n",
    "    # amostra pra não ficar pesado\n",
    "    dfx = df\n",
    "    try:\n",
    "        dfx = df.sample(withReplacement=False, fraction=sample_frac, seed=seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    stats = []\n",
    "    for c in string_cols:\n",
    "        s = (\n",
    "            dfx.select(\n",
    "                F.avg(F.length(F.trim(F.coalesce(F.col(c), F.lit(\"\"))))).alias(\"avg_len\"),\n",
    "                F.sum((F.length(F.trim(F.coalesce(F.col(c), F.lit(\"\")))) > 0).cast(\"int\")).alias(\"n_nonempty\")\n",
    "            )\n",
    "            .collect()[0]\n",
    "        )\n",
    "        avg_len = float(s[\"avg_len\"]) if s[\"avg_len\"] is not None else 0.0\n",
    "        n_nonempty = int(s[\"n_nonempty\"]) if s[\"n_nonempty\"] is not None else 0\n",
    "        stats.append((c, avg_len, n_nonempty))\n",
    "\n",
    "    # ordena por \"conteúdo\": primeiro n_nonempty, depois avg_len\n",
    "    stats_sorted = sorted(stats, key=lambda x: (x[2], x[1]), reverse=True)\n",
    "\n",
    "    print(\"\\n[Etapa 16.1] Top colunas textuais (c, avg_len, n_nonempty) — amostra:\")\n",
    "    for row in stats_sorted[:15]:\n",
    "        print(row)\n",
    "\n",
    "    chosen = [c for c, avg_len, n_nonempty in stats_sorted if n_nonempty > 0 and avg_len >= 10][:topn]\n",
    "    return chosen\n",
    "\n",
    "# vamos excluir ids, chaves e colunas muito “técnicas”\n",
    "exclude_apps = set([JOIN_KEY, \"app_id\", \"vaga_id\"])\n",
    "exclude_vagas = set([JOIN_KEY, \"app_id\", \"vaga_id\"])\n",
    "\n",
    "apps_text_cols = top_text_columns(df_join.select([c for c in df_apps.columns if c in df_join.columns]), exclude_cols=exclude_apps, topn=6)\n",
    "vagas_text_cols = top_text_columns(df_join.select([c for c in df_vagas.columns if c in df_join.columns]), exclude_cols=exclude_vagas, topn=6)\n",
    "\n",
    "print(f\"\\n[Etapa 16.1] Colunas escolhidas p/ candidate_text: {apps_text_cols}\")\n",
    "print(f\"[Etapa 16.1] Colunas escolhidas p/ job_text:       {vagas_text_cols}\")\n",
    "\n",
    "# fallback se não achou nada\n",
    "if not apps_text_cols:\n",
    "    print(\"[Etapa 16.1][WARN] Não achei colunas textuais úteis em df_apps dentro do join. Vou usar qualquer string col do join como fallback.\")\n",
    "    apps_text_cols = [c for c, t in df_join.dtypes if t==\"string\" and c not in exclude_apps][:3]\n",
    "\n",
    "if not vagas_text_cols:\n",
    "    print(\"[Etapa 16.1][WARN] Não achei colunas textuais úteis em df_vagas dentro do join. Vou usar qualquer string col do join como fallback.\")\n",
    "    vagas_text_cols = [c for c, t in df_join.dtypes if t==\"string\" and c not in exclude_vagas][:3]\n",
    "\n",
    "# -------------------------\n",
    "# 16.1c — Construir textos finais\n",
    "# -------------------------\n",
    "df_join = df_join.withColumn(\n",
    "    \"candidate_text\",\n",
    "    F.trim(F.concat_ws(\"\\n\", *[F.coalesce(F.col(c).cast(\"string\"), F.lit(\"\")) for c in apps_text_cols]))\n",
    ")\n",
    "\n",
    "df_join = df_join.withColumn(\n",
    "    \"job_text\",\n",
    "    F.trim(F.concat_ws(\"\\n\", *[F.coalesce(F.col(c).cast(\"string\"), F.lit(\"\")) for c in vagas_text_cols]))\n",
    ")\n",
    "\n",
    "print(\"\\n[Etapa 16.1] Preview tamanhos dos textos (deve ser > 0):\")\n",
    "df_join.select(\n",
    "    \"app_id\",\"vaga_id\",\n",
    "    F.length(\"candidate_text\").alias(\"len_candidate_text\"),\n",
    "    F.length(\"job_text\").alias(\"len_job_text\")\n",
    ").show(10, truncate=False)\n",
    "\n",
    "# sanity check: se job_text continuar 0, não faz sentido seguir\n",
    "agg = df_join.select(\n",
    "    F.mean(F.length(\"candidate_text\")).alias(\"mean_len_candidate\"),\n",
    "    F.mean(F.length(\"job_text\")).alias(\"mean_len_job\"),\n",
    "    F.sum((F.length(\"job_text\") > 0).cast(\"int\")).alias(\"n_job_nonempty\"),\n",
    "    F.count(\"*\").alias(\"n\")\n",
    ").collect()[0]\n",
    "\n",
    "print(\"\\n[Etapa 16.1] Sanity:\")\n",
    "print(dict(agg.asDict()))\n",
    "\n",
    "if int(agg[\"n_job_nonempty\"]) == 0:\n",
    "    raise ValueError(\n",
    "        \"job_text continua vazio para 100% das linhas. Isso indica que df_vagas não trouxe campos de descrição/requisitos no join.\\n\"\n",
    "        \"➡️ Solução: precisamos ajustar quais colunas de texto existem em df_vagas (fora do join) ou corrigir a chave de join (provável).\"\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# 16.1d — Modelo BERT\n",
    "# -------------------------\n",
    "model = SentenceTransformer(MODEL_NAME, device=DEVICE)\n",
    "\n",
    "def encode_texts(pdf: pd.DataFrame, text_col: str, id_col: str, batch_size: int = 64) -> pd.DataFrame:\n",
    "    texts = pdf[text_col].fillna(\"\").astype(str).tolist()\n",
    "    ids = pdf[id_col].tolist()\n",
    "\n",
    "    non_empty_texts, non_empty_pos = [], []\n",
    "    for i, t in enumerate(texts):\n",
    "        if t.strip():\n",
    "            non_empty_texts.append(t)\n",
    "            non_empty_pos.append(i)\n",
    "\n",
    "    vecs = None\n",
    "    if non_empty_texts:\n",
    "        vecs = model.encode(\n",
    "            non_empty_texts,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        ).astype(np.float32)\n",
    "\n",
    "    out_emb = [None] * len(texts)\n",
    "    if vecs is not None:\n",
    "        for j, i in enumerate(non_empty_pos):\n",
    "            out_emb[i] = vecs[j].tolist()\n",
    "\n",
    "    return pd.DataFrame({id_col: ids, \"embedding\": out_emb})\n",
    "\n",
    "def build_embeddings_cache(df_unique, id_col, text_col, cache_path):\n",
    "    # sempre sobrescreve aqui porque estamos corrigindo texto\n",
    "    if os.path.exists(cache_path):\n",
    "        os.remove(cache_path)\n",
    "\n",
    "    pdf = df_unique.select(id_col, text_col).toPandas()\n",
    "    emb_pdf = encode_texts(pdf, text_col=text_col, id_col=id_col, batch_size=64)\n",
    "    emb_pdf.to_parquet(cache_path, index=False)\n",
    "    return emb_pdf\n",
    "\n",
    "# uniques\n",
    "apps_unique = df_join.select(\"app_id\",\"candidate_text\").dropDuplicates([\"app_id\"])\n",
    "jobs_unique = df_join.select(\"vaga_id\",\"job_text\").dropDuplicates([\"vaga_id\"])\n",
    "\n",
    "print(f\"\\n[Etapa 16.1] Únicos: apps={apps_unique.count()} | jobs={jobs_unique.count()}\")\n",
    "\n",
    "# embeddings (rebuild)\n",
    "print(f\"[Etapa 16.1] Recriando embeddings apps => {APP_EMB_PATH}\")\n",
    "apps_emb_pdf = build_embeddings_cache(apps_unique, \"app_id\", \"candidate_text\", APP_EMB_PATH)\n",
    "\n",
    "print(f\"[Etapa 16.1] Recriando embeddings jobs => {JOB_EMB_PATH}\")\n",
    "jobs_emb_pdf = build_embeddings_cache(jobs_unique, \"vaga_id\", \"job_text\", JOB_EMB_PATH)\n",
    "\n",
    "# volta pro spark\n",
    "apps_emb = spark.createDataFrame(apps_emb_pdf)\n",
    "jobs_emb = spark.createDataFrame(jobs_emb_pdf)\n",
    "\n",
    "# cosine (dot; embeddings normalizados)\n",
    "@F.udf(returnType=T.FloatType())\n",
    "def cosine_sim(u, v):\n",
    "    if u is None or v is None:\n",
    "        return float(\"nan\")\n",
    "    a = np.array(u, dtype=np.float32)\n",
    "    b = np.array(v, dtype=np.float32)\n",
    "    return float(np.dot(a, b))\n",
    "\n",
    "df_scored = (\n",
    "    df_join\n",
    "    .join(apps_emb.withColumnRenamed(\"embedding\",\"emb_app\"), on=\"app_id\", how=\"left\")\n",
    "    .join(jobs_emb.withColumnRenamed(\"embedding\",\"emb_job\"), on=\"vaga_id\", how=\"left\")\n",
    "    .withColumn(\"sim_score_bert\", cosine_sim(F.col(\"emb_app\"), F.col(\"emb_job\")))\n",
    ")\n",
    "\n",
    "df_scored = df_scored.withColumn(\n",
    "    \"sim_score_bert\",\n",
    "    F.when(F.isnan(\"sim_score_bert\") | F.col(\"sim_score_bert\").isNull(), F.lit(0.0)).otherwise(F.col(\"sim_score_bert\"))\n",
    ")\n",
    "\n",
    "print(\"\\n[Etapa 16.1] Preview sim_score_bert (agora deve variar):\")\n",
    "df_scored.select(\"app_id\",\"vaga_id\",\"sim_score_bert\").show(10, truncate=False)\n",
    "\n",
    "print(\"\\n[Etapa 16.1] Distribuição:\")\n",
    "df_scored.select(\n",
    "    F.count(\"*\").alias(\"n\"),\n",
    "    F.mean(\"sim_score_bert\").alias(\"mean\"),\n",
    "    F.expr(\"percentile(sim_score_bert, array(0.1,0.5,0.9))\").alias(\"p10_p50_p90\")\n",
    ").show(truncate=False)\n",
    "\n",
    "# salvar scored\n",
    "df_scored.write.mode(\"overwrite\").parquet(OUT_SCORED_PATH)\n",
    "print(f\"\\nOK ✅ Salvo: {OUT_SCORED_PATH}\")\n",
    "\n",
    "globals()[\"df_scored\"] = df_scored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbb48a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "Ms28yc7ts3dS",
    "outputId": "030e56e4-ec81-4283-db3c-a15b0b1a6c42"
   },
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# ETAPA 16.2 — CV x VAGA \"de verdade\"\n",
    "# Cria candidate_text a partir de df_apps e job_text a partir de df_vagas (fora do join)\n",
    "# Depois calcula embeddings e similaridade e junta no df_final\n",
    "# =========================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "CACHE_DIR = \"/content/nlp_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "APP_EMB_PATH = os.path.join(CACHE_DIR, \"app_embeddings.parquet\")\n",
    "JOB_EMB_PATH = os.path.join(CACHE_DIR, \"job_embeddings.parquet\")\n",
    "OUT_SCORED_PATH = os.path.join(CACHE_DIR, \"df_scored_with_bert.parquet\")\n",
    "\n",
    "print(f\"[Etapa 16.2] DEVICE={DEVICE} | MODEL={MODEL_NAME} | CACHE={CACHE_DIR}\")\n",
    "\n",
    "df_apps = globals().get(\"df_apps\", None)\n",
    "df_vagas = globals().get(\"df_vagas\", None)\n",
    "if df_apps is None or df_vagas is None:\n",
    "    raise ValueError(\"Não encontrei df_apps e/ou df_vagas. Rode as etapas que criam esses DataFrames.\")\n",
    "\n",
    "JOIN_KEY = \"__root_id\"\n",
    "if JOIN_KEY not in df_apps.columns or JOIN_KEY not in df_vagas.columns:\n",
    "    raise ValueError(f\"JOIN_KEY '{JOIN_KEY}' precisa existir em df_apps e df_vagas.\")\n",
    "\n",
    "# -------------------------\n",
    "# helper: achar melhores colunas textuais em um DF\n",
    "# -------------------------\n",
    "def top_text_columns(df, exclude_cols=set(), topn=8, sample_frac=0.20, seed=42):\n",
    "    string_cols = [c for c, t in df.dtypes if t == \"string\" and c not in exclude_cols]\n",
    "    if not string_cols:\n",
    "        return []\n",
    "\n",
    "    dfx = df\n",
    "    try:\n",
    "        dfx = df.sample(withReplacement=False, fraction=sample_frac, seed=seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    stats = []\n",
    "    for c in string_cols:\n",
    "        s = (\n",
    "            dfx.select(\n",
    "                F.avg(F.length(F.trim(F.coalesce(F.col(c), F.lit(\"\"))))).alias(\"avg_len\"),\n",
    "                F.sum((F.length(F.trim(F.coalesce(F.col(c), F.lit(\"\")))) > 0).cast(\"int\")).alias(\"n_nonempty\")\n",
    "            ).collect()[0]\n",
    "        )\n",
    "        avg_len = float(s[\"avg_len\"]) if s[\"avg_len\"] is not None else 0.0\n",
    "        n_nonempty = int(s[\"n_nonempty\"]) if s[\"n_nonempty\"] is not None else 0\n",
    "        stats.append((c, avg_len, n_nonempty))\n",
    "\n",
    "    stats_sorted = sorted(stats, key=lambda x: (x[2], x[1]), reverse=True)\n",
    "\n",
    "    print(\"\\n[Etapa 16.2] Top colunas textuais (c, avg_len, n_nonempty) — amostra:\")\n",
    "    for row in stats_sorted[:20]:\n",
    "        print(row)\n",
    "\n",
    "    chosen = [c for c, avg_len, n_nonempty in stats_sorted if n_nonempty > 0 and avg_len >= 30][:topn]\n",
    "    return chosen\n",
    "\n",
    "# -------------------------\n",
    "# 16.2a — candidate_text (apps)\n",
    "# -------------------------\n",
    "# seu output mostrou cv_pt como a melhor: vamos manter\n",
    "if \"cv_pt\" not in df_apps.columns:\n",
    "    raise ValueError(\"Não achei coluna 'cv_pt' em df_apps. Me diga qual coluna tem o texto do CV.\")\n",
    "\n",
    "apps_text_cols = [\"cv_pt\"]\n",
    "df_apps_text = (\n",
    "    df_apps\n",
    "    .select(JOIN_KEY, *apps_text_cols)\n",
    "    .withColumn(\"candidate_text\", F.trim(F.concat_ws(\"\\n\", *[F.coalesce(F.col(c), F.lit(\"\")) for c in apps_text_cols])))\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 16.2b — job_text (vagas) REAL (descobrir automaticamente no df_vagas)\n",
    "# -------------------------\n",
    "exclude_vagas = {JOIN_KEY}\n",
    "vagas_text_cols = top_text_columns(df_vagas, exclude_cols=exclude_vagas, topn=10)\n",
    "\n",
    "if not vagas_text_cols:\n",
    "    raise ValueError(\n",
    "        \"Não encontrei colunas textuais úteis em df_vagas (avg_len>=30 e n_nonempty>0).\\n\"\n",
    "        \"➡️ Me mande df_vagas.printSchema() ou df_vagas.columns pra eu mapear os campos de descrição/requisitos.\"\n",
    "    )\n",
    "\n",
    "df_vagas_text = (\n",
    "    df_vagas\n",
    "    .select(JOIN_KEY, *vagas_text_cols)\n",
    "    .withColumn(\"job_text\", F.trim(F.concat_ws(\"\\n\", *[F.coalesce(F.col(c), F.lit(\"\")) for c in vagas_text_cols])))\n",
    ")\n",
    "\n",
    "print(f\"\\n[Etapa 16.2] candidate_text cols: {apps_text_cols}\")\n",
    "print(f\"[Etapa 16.2] job_text cols:       {vagas_text_cols}\")\n",
    "\n",
    "print(\"\\n[Etapa 16.2] Preview tamanhos:\")\n",
    "(\n",
    "    df_apps_text.select(F.length(\"candidate_text\").alias(\"len_candidate\")).summary(\"count\",\"mean\",\"min\",\"max\").show()\n",
    ")\n",
    "(\n",
    "    df_vagas_text.select(F.length(\"job_text\").alias(\"len_job\")).summary(\"count\",\"mean\",\"min\",\"max\").show()\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 16.2c — Embeddings\n",
    "# -------------------------\n",
    "model = SentenceTransformer(MODEL_NAME, device=DEVICE)\n",
    "\n",
    "def encode_texts(pdf: pd.DataFrame, text_col: str, id_col: str, batch_size: int = 64) -> pd.DataFrame:\n",
    "    texts = pdf[text_col].fillna(\"\").astype(str).tolist()\n",
    "    ids = pdf[id_col].tolist()\n",
    "\n",
    "    non_empty_texts, non_empty_pos = [], []\n",
    "    for i, t in enumerate(texts):\n",
    "        if t.strip():\n",
    "            non_empty_texts.append(t)\n",
    "            non_empty_pos.append(i)\n",
    "\n",
    "    vecs = None\n",
    "    if non_empty_texts:\n",
    "        vecs = model.encode(\n",
    "            non_empty_texts,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        ).astype(np.float32)\n",
    "\n",
    "    out_emb = [None] * len(texts)\n",
    "    if vecs is not None:\n",
    "        for j, i in enumerate(non_empty_pos):\n",
    "            out_emb[i] = vecs[j].tolist()\n",
    "\n",
    "    return pd.DataFrame({id_col: ids, \"embedding\": out_emb})\n",
    "\n",
    "def rebuild_embeddings(df_unique, id_col, text_col, cache_path):\n",
    "    if os.path.exists(cache_path):\n",
    "        os.remove(cache_path)\n",
    "    pdf = df_unique.select(id_col, text_col).toPandas()\n",
    "    emb_pdf = encode_texts(pdf, text_col=text_col, id_col=id_col, batch_size=64)\n",
    "    emb_pdf.to_parquet(cache_path, index=False)\n",
    "    return emb_pdf\n",
    "\n",
    "apps_unique = df_apps_text.select(JOIN_KEY, \"candidate_text\").dropDuplicates([JOIN_KEY])\n",
    "jobs_unique = df_vagas_text.select(JOIN_KEY, \"job_text\").dropDuplicates([JOIN_KEY])\n",
    "\n",
    "print(f\"\\n[Etapa 16.2] Únicos: apps={apps_unique.count()} | jobs={jobs_unique.count()}\")\n",
    "\n",
    "print(f\"[Etapa 16.2] Recriando embeddings apps => {APP_EMB_PATH}\")\n",
    "apps_emb_pdf = rebuild_embeddings(apps_unique, JOIN_KEY, \"candidate_text\", APP_EMB_PATH)\n",
    "\n",
    "print(f\"[Etapa 16.2] Recriando embeddings jobs => {JOB_EMB_PATH}\")\n",
    "jobs_emb_pdf = rebuild_embeddings(jobs_unique, JOIN_KEY, \"job_text\", JOB_EMB_PATH)\n",
    "\n",
    "apps_emb = spark.createDataFrame(apps_emb_pdf).withColumnRenamed(\"embedding\",\"emb_app\")\n",
    "jobs_emb = spark.createDataFrame(jobs_emb_pdf).withColumnRenamed(\"embedding\",\"emb_job\")\n",
    "\n",
    "@F.udf(returnType=T.FloatType())\n",
    "def cosine_sim(u, v):\n",
    "    if u is None or v is None:\n",
    "        return float(\"nan\")\n",
    "    a = np.array(u, dtype=np.float32)\n",
    "    b = np.array(v, dtype=np.float32)\n",
    "    return float(np.dot(a, b))\n",
    "\n",
    "df_scored = (\n",
    "    df_apps_text\n",
    "    .select(JOIN_KEY, \"candidate_text\")\n",
    "    .join(df_vagas_text.select(JOIN_KEY, \"job_text\"), on=JOIN_KEY, how=\"inner\")\n",
    "    .join(apps_emb, on=JOIN_KEY, how=\"left\")\n",
    "    .join(jobs_emb, on=JOIN_KEY, how=\"left\")\n",
    "    .withColumn(\"sim_score_bert\", cosine_sim(F.col(\"emb_app\"), F.col(\"emb_job\")))\n",
    ")\n",
    "\n",
    "df_scored = df_scored.withColumn(\n",
    "    \"sim_score_bert\",\n",
    "    F.when(F.isnan(\"sim_score_bert\") | F.col(\"sim_score_bert\").isNull(), F.lit(0.0)).otherwise(F.col(\"sim_score_bert\"))\n",
    ")\n",
    "\n",
    "print(\"\\n[Etapa 16.2] Distribuição sim_score_bert (agora deve fazer sentido):\")\n",
    "df_scored.select(\n",
    "    F.count(\"*\").alias(\"n\"),\n",
    "    F.mean(\"sim_score_bert\").alias(\"mean\"),\n",
    "    F.expr(\"percentile(sim_score_bert, array(0.1,0.5,0.9))\").alias(\"p10_p50_p90\")\n",
    ").show(truncate=False)\n",
    "\n",
    "print(\"\\n[Etapa 16.2] Preview:\")\n",
    "df_scored.select(F.length(\"candidate_text\").alias(\"len_c\"), F.length(\"job_text\").alias(\"len_j\"), \"sim_score_bert\").show(10)\n",
    "\n",
    "df_scored.write.mode(\"overwrite\").parquet(OUT_SCORED_PATH)\n",
    "print(f\"\\nOK ✅ Salvo: {OUT_SCORED_PATH}\")\n",
    "\n",
    "globals()[\"df_scored\"] = df_scored\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
